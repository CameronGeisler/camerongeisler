<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="section-GEAM">
    <title>Gaussian Elimination and Matrices</title>

    <introduction>
        Then, Gaussian elimination can be reframed in terms of modifying rows of the augmented matrix of a system.
    </introduction>

    <subsection>
        <title>Elementary Row Operations</title>
        <definition>
            <title>Elementary Row Operation</title>
            <ul>
                <li><term>Interchange</term>. Interchange two rows in the matrix.</li>
                <li><term>Scaling</term>. Multiply (or divide) an equation by a non-zero real number.</li>
                <li><term>Replacement</term>. Add to one equation a multiple of another equation.</li>
            </ul>
            <p>
                These row operations are denoted symbolically by,
            </p>
            <ul>
                <li><m>R_i \leftrightarrow R_j</m>, interchanging row <m>i</m> and row <m>j</m>.</li>
                <li><m>kR_i \rightarrow R_i</m></li>
                <li><m>R_i + k R_j \rightarrow R_i</m></li>
            </ul>
        </definition>

        <p>
            Performing Gaussian elimination with this matrix form removes the need of repeatedly writing the variables. This process on a matrix is also called <term>row reduction</term>.
        </p>

        <ul>
            <li>Two matrices are <term>row equivalent</term> if one can be obtained from the other by a (finite) sequence of row oeprations.</li>
            <li>A <term>zero</term> row has only zeros. A <term>non-zero</term> row is a row with at least one non-zero entry.</li>
            <li>The <term>leading entry</term> of a non-zero row is the left-most non-zero entry. It is also called the <term>leading coefficient</term>, or <term>pivot</term>.</li>
        </ul>
    </subsection>

    <subsection>
        <title>Row Echelon Form</title>

        <p>
            After Gaussian elimination is performed on a matrix, the staircase or triangular form of the matrix is called row echelon form.
        </p>

        <definition>
            A matrix is in <term>row echelon form</term> (or simply <term>echelon form</term>, or <term>REF</term>) if,
            <ol>
                <li>Any zero rows are the bottom of the matrix.</li>
                <li>the leading entry of every non-zero row is always strictly to the right of any leading entry above it.</li>
            </ol>
        </definition>

        <p>
            Matrices in row echelon form have the broad form,
        </p>
        <me>
            \begin{bmatrix} a \amp \ast \amp \ast \amp \ast \amp \ast \amp \ast \\
            0 \amp b \amp \ast \amp \ast \amp \ast \amp \ast \\
            0 \amp 0 \amp 0 \amp c \amp \ast \amp \ast \\
            0 \amp 0 \amp 0 \amp 0 \amp 0 \amp d \\
            0 \amp 0 \amp 0 \amp 0 \amp 0 \amp 0 \end{bmatrix}
        </me>
        where the pivots <m>a, b, c, d</m> are non-zero numbers. Notice that,
        <ul>
            <li>Every entry below a pivot is 0.</li>
            <li>The leading entries are <q>staggered to the right</q> as we go down.</li>
            <li>All zero rows are at the bottom.</li>
        </ul>

        <p>
            The terminology <em>echelon</em> is due to the step-like pattern of the non-zero entries of the matrix. The process of transforming a matrix into row echelon form is called <term>row reduction</term>. In summary,
        </p>

        <me>
            \begin{pmatrix} \text{doing operations on a} \\ \text{system of equations} \\ \text{to convert it to triangular form} \end{pmatrix} \quad \iff \quad \begin{pmatrix} \text{doing row operations on a} \\ \text{augmented matrix} \\ \text{to convert it to row echelon form} \end{pmatrix}
        </me>

        <definition>
            A <term>pivot position</term> in a matrix <m>A</m> is a location which corresponds to a leading entry in the row echelon form of <m>A</m>. A <term>pivot column</term> is a column of <m>A</m> which contains a pivot position.
        </definition>
    </subsection>

    <subsection>
        <title>Summary of Row Reduction</title>

        <p>
            Consider a matrix <m>A</m>. Broadly, row reduction involves going column by column, reducing the matrix to a simpler matrix at each step.
        </p>
        <ol>
            <li>
                First, determine the left-most non-zero column, which will be the first <em>pivot column</em>. In this column, choose a non-zero entry to be the <em>pivot entry</em>. If necessary, use a row exchange to positive the pivot at the top of the column.
                <me>
                    \begin{bmatrix} \mid \amp \mid \amp a \amp \ast \amp \ast \amp \ast \\
                    0 \amp 0 \amp \ast \amp \ast \amp \ast \amp \ast \\
                    \mid \amp \mid \amp \ast \amp \ast \amp \ast \amp \ast \end{bmatrix}
                </me>
                In terms of completing the algorithm, it doesn't matter which non-zero entry we select to be the pivot. For hand-computation, it is typically easier to select a 1 if that is available, as that will make the computations a bit more simple.
            </li>

            <li>
                Use row replacement operations to create zeros in all positions below the pivot. That is, add multiples of the top row to rows below it as to remove all entries below the pivot entry.
                <me>
                    \begin{bmatrix} \mid \amp \mid \amp a \amp \ast \amp \ast \amp \ast \\
                    0 \amp 0 \amp 0 \amp \ast \amp \ast \amp \ast \\
                    \mid \amp \mid \amp 0 \amp \ast \amp \ast \amp \ast \end{bmatrix}
                </me>
            </li>
            <li>
                Repeat the previous steps with the remaining submatrix below and to the right of the pivot,
                <me>
                    \begin{bmatrix}
                    \mid \amp \mid \amp a \amp \ast \amp \ast \amp \ast \\
                    0 \amp 0 \amp 0 \amp b \amp \ast \amp \ast \\
                    \mid \amp \mid \amp 0 \amp 0 \amp \ast \amp \ast
                    \end{bmatrix} \qquad \longrightarrow \qquad \underbrace{\begin{bmatrix} b \amp \ast \amp \ast \\ 0 \amp \ast \amp \ast \end{bmatrix}}_{\text{submatrix}}
                </me>
                (In this way, row reduction is a recursive algorithm, as row reduction of a matrix depends on performing row reduction on some smaller matrix). Repeat this step until there are no more non-zero rows to modify.                
            </li>
        </ol>
        <p>
            Finally, to convert the matrix to RREF,
        </p>
        <ol start="4">
            <li>Starting with the right-most pivot, use replacement row operations to create zeros above each pivot. If a pivot is not 1, make it 1 using a scaling operation. </li>
        </ol>
        <p>
            Steps 1-3 to convert a matrix to REF is called the <term>forward phase</term> of Gaussian elimination. Step 4 to convert it to the unique RREF is called the <term>backwards phase</term>.
        </p>
    </subsection>

    <subsection>
        <title>Remark on Gaussian Elimination</title>
        <p>
            Gaussian elimination, despite its simplicity, is one of the most important algorithms in linear algebra. Notice that Gaussian elimination is not really much more than high school algebra, but done in a systematic way.
        </p>
    </subsection>

    <subsection>
        <title>Systems with Infinitely Many Solutions</title>
        <p>
            If the REF of the augmented matrix of a system has a zero row, then the system has infinitely many solutions.
        </p>

        <example>
            <p>
                Consider the matrix in REF,
            </p>
            <me>
                \begin{bmatrix}
                1 \amp 0 \amp -5 \amp 1 \\
                0 \amp 1 \amp 1 \amp 4 \\
                0 \amp 0 \amp 0 \amp 0
                \end{bmatrix}
            </me>
            which is the augmented matrix of the linear system,
            <me>
                \begin{array}{rrrrr}
                x \amp \amp -5z \amp = \amp 1 \\
                \amp y \amp + z \amp = \amp 4 \\
                \amp \amp 0 \amp = \amp 0
                \end{array}
            </me>
            At this point, notice that the third equation is the true statement (or tautology) <m>0 = 0</m>. Intuitively, this means that while there were originally 3 equations that constrained the possible solutions, one of those equations were superfluous. Then, any solution <m>(x,y,z)</m> only has to satisfy the remaining two equations. In short, there are 3 variables, but only 2 constraints. In this case, for example, if we choose an arbitrary value of <m>z</m>, say <m>z = t</m>, then we can solve for <m>x</m> and <m>y</m> in equation 1 and 2, respectively, and get a solution of the system. In particular,
            <me>
                \begin{cases}
                x = 1 + 5t \\
                y = 4 - t \\
                z = t
                \end{cases}
            </me>
            Then, any tuple of the form <m>(1+5t,4-t,t)</m> (where <m>t \in \mathbb{R}</m> is any number) will be a solution to the system. In particular, there are infinitely many solutions of the system. Here, <m>z</m> is called a <em>free variable</em>, because it can take on any value to generate a solution of the system.
        </example>

        <definition>
            Variables which correspond to pivot columns are called <term>basic variables</term>. The other variables are called <term>free variables</term>.
        </definition>
        
        <p>
            In general, for the augmented matrix of a system in REF, any non-pivot column corresponds to a free variable (besides the right-most constant column). Free variables can take on any value, and then we can solve for the basic variables in terms of the free variables.
        </p>
    </subsection>

    <subsection>
        <title>Systems with No Solution</title>

        <p>
            If the REF of the augmented matrix of a system has a row which corresponds to a false statement, then the system has no solution.
        </p>

        <example>
            Consider the matrix in REF,
            <me>
                \begin{bmatrix}
                1 \amp -3 \amp 1 \amp 4 \\
                0 \amp -1 \amp -4 \amp 7 \\
                0 \amp 0 \amp 0 \amp 2
                \end{bmatrix}
            </me>
            which corresponds to the linear system,
            <me>
                \begin{array}{rrrrr}
                    x \amp -3y \amp + z \amp = \amp 1 \\
                    \amp -y \amp -4 z \amp = \amp 4 \\
                    \amp \amp 0 \amp = \amp 2
                \end{array}
            </me>
            The 3rd equation is the false statement (or, <em>contradiction</em>) <m>0 = 2</m>, so the system has no solution. This is because, no matter what the values of <m>x, y, z</m>, it is not possible for the third equation to be satisfied. A solution of the system must make every equation true, so no such solution exists. Thus, the system is inconsistent.
        </example>
    </subsection>

    <subsection>
        <title>Reduced Row Echelon Form</title>
        
        <p>
            The row ecehlon form of a matrix is not unique, in that, by doing different sequences of row operations, the final row echelon forms can be different. However, we can continue to apply row operations in order to simplify the matrix further into <em>reduced</em> row ecehlon form.
        </p>

        <definition>
            A matrix is in <term>reduced row echelon form</term> (<term>RREF</term>) if,
            <ol>
                <li>The matrix is in REF.</li>
                <li>Every pivot is 1.</li>
                <li>Every pivot is the only non-zero entry in its column.</li>
            </ol>
        </definition>

        <p>
            In short, RREF goes further than REF by requiring that every pivot is 1 (rather than just any number), and that there are all 0's below <em>and</em> above each pivot. 
        </p>

        <example>
            <p>A classic example of a matrix in RREF is a matrix of the form,</p>
            <me>
                \begin{bmatrix} 1 \amp 0 \amp 0 \amp a \\ 0 \amp 1 \amp 0 \amp b \\ 0 \amp 0 \amp 1 \amp c \end{bmatrix}
            </me>
            <p>
                This represents a system of 3 linear equations in 3 unknowns. In particular the system,
            </p>

            <me>
                \begin{array}{rrrrr}
                1x \amp + 0y \amp + 0z \amp = \amp a \\
                0x \amp + 1y \amp + 0z \amp = \amp b \\
                0x \amp + 0y \amp + 1z \amp = \amp c
                \end{array}
            </me>
            In other words,
            <me>
                \begin{array}{rrr}
                x \amp = \amp a \\
                y \amp = \amp b \\
                z \amp = \amp c \end{array}
            </me>
            <p>
                from which the unique solution can be read right off as <m>(a,b,c)</m>.
            </p>
        </example>

        <p>
            Basically, RREF <q>builds in</q> the back substitution into the row reduction procedure. 
        </p>

        <example>
            Consider the system,
            <me>
                \begin{array}{rrrrr}
                2x \amp + y \amp + z \amp = \amp 1 \\
                4x \amp + y \amp \amp = \amp -2 \\
                -2x \amp + 2y \amp + z \amp = \amp 7
                \end{array}
            </me>
            which has augmented matrix,
            <me>
                \left[\begin{array}{ccc|c} 2 \amp 1 \amp 1 \amp 1 \\ 4 \amp 1 \amp 0 \amp -2 \\ -2 \amp 2 \amp 1 \amp 7 \end{array}\right]
            </me>
            After reducing to REF, one possible form is,
            <me>
                \left[\begin{array}{ccc|c} 2 \amp 1 \amp 1 \amp 1 \\ 0 \amp -1 \amp -2 \amp -4 \\ 0 \amp 0 \amp -4 \amp -4 \end{array}\right]
            </me>
            We could perform back substitution as before, but we can also instead convert to RREF. Doing so gives,
            <me>
                \left[\begin{array}{ccc|c} 1 \amp 0 \amp 0 \amp -1 \\ 0 \amp 1 \amp 0 \amp 2 \\ 0 \amp 0 \amp 1 \amp 1 \end{array}\right]
            </me>
            from which the unique solution <m>(-1,2,1)</m> can be read right off.
        </example>

        <theorem>
            <title>Uniqueness of RREF</title>
            <p>
                Every matix is row equivalent to a unique matrix in RREF.
            </p>
        </theorem>

        <proof>
            <em>EXERCISE</em>.
        </proof>
    
    <p>
        The process of converting a matrix into RREF is sometimes called <term>Gauss-Jordan elimination</term>, due to Gauss and also Wilhelm Jordan.
    </p>
    </subsection>

    <subsection>
        <title>Existence and Uniqueness, Summary</title>

        <theorem>
            <p>
                A linear system is consistent if and only if the right-most column of its augmented matrix is not a pivot column. In other words, if and only if the REF of the augmented matrix has no row of the form,
            </p>
            <me>
                \begin{bmatrix} 0 \amp \dots \amp 0 \amp b \end{bmatrix} \qquad \text{where <m>b \neq 0</m>}
            </me>
            <p>
                If a linear system in consistent, then the solution set is either:
            </p>
            <ol>
                <li>A unique solution, if there are no free variables.</li>
                <li>Infinitely many solutions, if there are at least one free variable.</li>
            </ol>
        </theorem>

    </subsection>

    <subsection>
        <title>Summary of Solving Systems with Gaussian Elimination</title>
        <ol>
            <li>Determine the augmented matrix of the system <m>\begin{bmatrix} A \mid b \end{bmatrix}</m>.</li>
            <li>Convert the matrix into an equivalent matrix in REF, using row reduction. If the system is inconsistent, then we are done.</li>
            <li>If the system is consistent, then convert the matrix into RREF.</li>
            <li>Write the system of equations corresponding to the matrix in RREF.</li>
            <li>Solve for each basic variable in terms of any free variables.</li>
        </ol>
    </subsection>

    <subsection>
        <title>Historical Note</title>
        <p>
            Gaussian elimination is named after German mathematician <term>Carl Friedrich Gauss</term> (1777-1855), who developed the method in his work and helped develop the formal theory of matrices. German engineer <term>Wilhelm Jordan</term> (1842-1899) helped popularize the method.
        </p>

        <p>
            However, similar methods of elimination were developed as early as around 250-100 BC by the ancient Chinese, during the Han dynasty. The method is included in the book <q>The Nine Chapters on the Mathematical Art</q> (Jiuzhang suanshu), one of the earliest surviving mathematical texts from China. The ancient chinese also had a device called a counting board, which contained the array of coefficients, and allowed for calculations similar to that with matrices.
        </p>
    </subsection>

        
    <subsection>
        <title>Number of Operations for Row Reduction (Flops)</title>
        <example>
            <p>
                Consider the number of operations required to perform row reduction. Let <m>A</m> be an <m>n \times n</m> matrix. To consider the worst case, we will assume that <m>A</m> has all non-zero entries (if the matrix has some zero entries, this reduces the computations required).
            </p>
            <p>
                The first step of elimination requires making zeros in the first column, through row replacement operations. Each row replacement operation (<m>R_i + k R_j \rightarrow R_i</m>) basically involves a single multiplication (of <m>k</m> times an entry in <m>R_j</m>) and a single addition (<m>R_i + k R_j</m>). For simplicity, we will assume that each operation (addition, subtraction, multiplication, division) all take roughly the same amount of time. 
            </p>
            <p>
                We call a single one of these operations a <term>flop</term> (short for <em>floating point operation</em>). Traditionally, flop referred only to a multiplication or division, because these operations took somewhat more time and the more simple operations of addition and subtraction, and so the latter could be ignored, relatively speaking. With modern technology, all operations take about the same time.
            </p>
            <p>
                Then, modifying a single entry with a replacement row operation requires 2 flops. Then, the first step in row reduction requires modifying <m>n-1</m> rows of the matrix (every row except the pivot row), and each of these rows has <m>n</m> entries. So, the first step requires <m>2n(n-1)</m> flops.
            </p>
            <p>
                The second step involves a submatrix of size <m>(n-1) \times (n-1)</m>. Here, row reduction involves <m>n-2</m> rows, each with <m>n-1</m> entries, for a total for <m>2(n-1)(n-2)</m> flops.
            </p>
            <p>
                Continuing in this way, the subsequent steps require <m>2(n-2)(n-3), 2(n-3)(n-4), \dots</m> steps. The final step, acting on a <m>2 \times 2</m> submatrix, requries <m>2 \cdot 2 \cdot 1</m> flops. Thus, the total number of flops is,
            </p>
            <me>
                = 2n(n-1) + 2(n-1)(n-2) + \dots + 2 \cdot 3 \cdot 2 + 2 \cdot 2 \cdot 1
            </me>
            <p>
                In summation notation, this can be written as (in increasing order),
            </p>
            <me>
                = \sum_{k=1}^{n-1} 2 k(k + 1)
            </me>
            <p>
                This sum can be evaluated to find an explicit formula,
            </p>
            <md>
                <mrow>\amp = 2 \sum_{k=1}^{n-1} (k^2 + k)</mrow>
                <mrow>\amp = 2 \brac{\sum_{k=1}^{n-1} k^2 + \sum_{k=1}^{n-1} k</mrow>
            </md>
            <p>
                Then, we can apply the summation formulas,
            </p>
            <me>
                \sum_{k=1}^n k^2 = \frac{n(n+1)(2n+1)}{6} \qquad \sum_{k=1}^n k = \frac{n(n+1)}{2}
            </me>
            <p>
                Here, the upper limit of the summation is <m>n-1</m> rather than <m>n</m>, so we substitute <m>n-1</m> for <m>n</m> when applying the formulas,
            </p>
            <md>
                <mrow>\amp = 2 \brac{\frac{(n-1) \cdot n \cdot (2(n-1)+1)}{6} + \frac{(n-1) \cdot n}{2}}</mrow>
            </md>
            <p>
                Then, simplifying,
            </p>
            <md>
                <mrow>\amp = 2 \brac{\frac{n(n-1)(2n-1) + 3n(n-1)}{6}}</mrow>
                <mrow>\amp = 2 \brac{\frac{n(n-1)((2n-1)+3)}{6}}</mrow>
                <mrow>\amp = 2 \brac{\frac{n(n-1)(2n+2)}{6}}</mrow>
                <mrow>\amp = \frac{2n(n-1)(n+1)}{3}}</mrow>
            </md>
            <p>
                Then, consider the asymptotic behavior. When <m>n</m> is large, 
            </p>
            <me>
                \frac{2n(n-1)(n+1)}{3}} \approx \frac{2n^3}{3}
            </me>
            <p>
                In particular, about <m>n^3</m> operations.
            </p>
            <p>
                Next, consider back substitution. Here, the first step requires making a 0 everywhere above the bottom-right entry in the <m>n</m>th column. There are <m>n-1</m> entries, and again each uses a single row replacement, which is made up of 2 flops. So, <m>2(n-1)</m> flops. The next step involves <m>n-2</m> entries, and so has <m>2(n-2)</m> flops. Continuing in this way, the last step involving changing the single entry about the pivot in the <m>2nd</m> column, for <m>2 \cdot 1</m> flops. Then, in total, the number of flops is,
            </p>
            <md>
                <mrow>\amp = 2(n-1) + 2(n-2) + \dots + 2 \cdot 2 + 2 \cdot 1</mrow>
                <mrow>\amp = \sum_{k=1}^{n-1} 2k</mrow>
                <mrow>\amp = 2 \sum_{k=1}^{n-1} k</mrow>
                <mrow>\amp = 2 \cdot \frac{n(n-1)}{2}</mrow>
                <mrow>\amp = n(n-1)</mrow>
            </md>
            <p>
                As <m>n</m> becomes large, this is approximately <m>n^2</m> flops.
            </p>
        </example>
    </subsection>

    <subsection>
        <title>Computing Note</title>
        <p>
            A computer program performing row reduction with floating point numbers has to minimize rounding error when performing numerical operations. For this reason, they will typically choose pivot rows using not only by the necessary condition that the pivot entry is non-zero, but additionally it will choose the largest entry in the column, in absolute value. In other words, they will avoid using pivots which are close to 0.
        </p>
    </subsection>

    <exercises>
        <exercise>
            <statement>
                In Python, implement a Gaussian elimination algorithm, to be used on a matrix formed by a 2D array.
            </statement>
        </exercise>

        <exercise>
            <statement>
                In Python, extend the Gaussian elimination algorithm to perform Gauss-Jordan elimination.
            </statement>
        </exercise>
    </exercises>

</section>