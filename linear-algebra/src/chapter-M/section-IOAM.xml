<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="section-IOAM">
    <title>Inverse of a Matrix</title>

    <introduction>
        <p>
            Recall that for real numbers, the <em>inverse</em> (or <em>multiplicative inverse</em>, or <em>reciprocal</em> of <m>a \in \mathbb{R}</m> is the number <m>b</m> such that <a>ab = 1</a>. We denote <m>b</m> by <m>\frac{1}{a}</m> or <m>a^{-1}</m>. For example, the multiplicative inverse of <m>3</m> is <m>3^{-1} = \frac{1}{3}</m>, because <m>3 \cdot \frac{1}{3} = \frac{1}{3} \cdot 3 = 1</m>. Also, inverses only exist for non-zero real numbers.
        </p>
        <p>
            For matrices, we can define an analogous notion of inverse. However, unlike real number multiplication, matrix multiplication is not commutative. Further, for both sides of the product to be defined, the matrices involved must be square. So, most often, we focus on inverses of square matrices. Also, similarly, the inverse of a matrix will only exist if the matrix is <q>non-zero</q> in some sense.
        </p>
    </introduction>

    <subsection>
        <title>Inverse of a Matrix</title>
        <definition>
            An <m>n \times n</m> square matrix <m>A</m> is <term>invertible</term> if there exists an <m>n \times n</m> matrix <m>B</m> such that <m>AB = BA = I_n</m>. Then, <m>B</m> is called the <term>inverse</term> of <m>A</m>, and is denoted by <m>B = A^{-1}</m>.
        </definition>

        <theorem>
            <p>
                If a square matrix <m>A</m> is invertible, then its inverse is unique.
            </p>
        </theorem>
        <proof>
            If <m>B</m> and <m>C</m> are both inverses of <m>A</m>, then <m>AB = BA = I_n</m> and <m>AC = CA = I_n</m>. Then,
            <me>
                B = BI_n = B(AC) = (BA)C = I_n C = C
            </me>
            Thus, <m>B = C</m>.
        </proof>
        <p>
            Thus, the inverse of a matrix <m>A</m>, if it exists, is well-defined, and so is denoted by <m>A^{-1}</m>. It has the property that,
        </p>
        <me>
            AA^{-1} = A^{-1} A = I_n
        </me>

        <p>
            A matrix which is not invertible is sometimes called <term>singular</term>, and a matrix which is invertible is called <term>non-singular</term>.
        </p>
        <p>
            To verify that  matrix <m>B</m> is the inverse of <m>A</m>, we can multiply <m>B</m> by <m>A</m> and verify that the result is the identity matrix. Further, for a square matrix <m>A</m>, <m>AB = I_n</m> if and only if <m>BA = I_n</m>, so it is sufficient to only compute one of the two products <m>AB</m> or <m>BA</m>.
        </p>
    </subsection>
    
    <subsection>
        <title>Matrix Inverses and Linear Transformations</title>
        <p>
            Consider a matrix <m>A</m> as a linear transformation, its inverse <m>A^{-1}</m> is the inverse linear transformation in the sense of function inverse. That is, its composition with <m>A</m> results in the identity transformation.
        </p>
    </subsection>

    <subsection>
        <title>Inverse of a <m>2 \times 2</m> Matrix</title>
        <p>
            For a <m>2 \times 2</m> matrix, there is a simple test for invertibility, and an explicit formula for the inverse.
        </p>

        <theorem>
            <p>
                Let <m>A = \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}</m> be a <m>2 \times 2</m> matrix. If <m>ad - bc \neq 0</m>, then <m>A</m> is invertible, and its inverse is given by,
            </p>
            <me>
                \boxed{A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}}
            </me>
        </theorem>

        <p>
            If <m>ad - bc = 0</m>, then the matrix <m>A</m> does not have an inverse. Of course, a <q>proof</q> that this formula is correct can simply involve verifying the matrix multiplication that <m>AA^{-1} = I_2</m>. The method of actually determining the correct formula for <m>A^{-1}</m> from <m>A</m> is more complicated.
        </p>

        <proof>
            <md>
                <mrow>AA^{-1} \amp = \frac{1}{ad - bc} \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}</mrow>
                <mrow>\amp = \frac{1}{ad - bc} \begin{bmatrix} ad - bc \amp 0 \\ 0 \amp ad - bc \end{bmatrix}</mrow>
                <mrow>\amp = \begin{bmatrix} 1 \amp 0 \\ 0 \amp 1 \end{bmatrix} = I_2</mrow>
            </md>
        </proof>
    </subsection>

    <subsection>
        <title>Solving Systems with Inverse Matrices</title>

        <theorem>
            <p>
                If <m>A</m> is an <m>n \times n</m> matrix, then the matrix equation <m>A\vec{x} = \vec{b}</m> has a unique soloution (for any <m>\vec{b} \in \mathbb{R}^n</m>), given by the vector <m>\vec{x} = A^{-1} \vec{b}</m>.
            </p>
        </theorem>

        <p>
            Intuitively, this comes from multiplying both sides of the equation <m>A\vec{x} = \vec{b}</m> on the left by <m>A^{-1}</m>, to get <m>A^{-1}A \vec{x} = A^{-1} \vec{b}</m>.  Then, <m>A^{-1} A = I_n</m> and <m>I_n \vec{x} = \vec{x}</m>, so the equation becomes <m>\vec{x} = A^{-1} \vec{b}</m>. The uniqueness follows from the fact that matrix inverses are unique.
        </p>
        <proof>
            First, indeed <m>\vec{x} = A^{-1} \vec{b}</m> is a solution, as
            <me>
                A(A^{-1} \vec{b}) \amp = (AA^{-1}) \vec{b} = I_n \vec{b} = \vec{b}
            </me>
            so it indeed satisfies the equation. For uniqueness, let <m>\vec{u}</m> be any solution, so that <m>A\vec{u} = \vec{b}</m>. Then, multiplying on the left by <m>A^{-1}</m>, <m>A^{-1} A \vec{u} = A^{-1} \vec{b}</m>, or <m>I_n \vec{u} = A^{-1} \vec{b}</m>, and so <m>\vec{u} = A^{-1} \vec{b}</m>.
        </proof>
        <p>
            The formula <m>\vec{x} = A^{-1} \vec{b}</m> is rarely used in practice to solve a system of equations, because row reduction is almost always faster, especially for large systems, for which computing inverses is very time-consuming. A possible exception is for systems of two equations in two unknowns.
        </p>
        <p>
            The importance of this theorem is that says that invertibility guarentees a unique solution.
        </p>
    </subsection>

    <subsection>
        <title>Matrix Inverses and Linear Systems</title>
        <p>
            If the coefficient matrix does not have an inverse, then...
        </p>
    </subsection>

    <subsection>
        <title>Properties of Inverse Matrices</title>
        <theorem>
            <p>
                If <m>A</m> is invertible, then <m>A^{-1}</m> is invertible, and the inverse of <m>A^{-1}</m> is <m>A</m>, or
            </p>
            <me>
                (A^{-1})^{-1} = A
            </me>
        </theorem>

        <proof>
            By definition, the inverse of <m>A^{-1}</m> is the matrix <m>B</m> such that <m>A^{-1} B = BA^{-1} = I_n</m>. Clearly, <m>A</m> satisfies this, as <m>A^{-1} A = A A^{-1} = I_n</m>.
        </proof>

        <theorem>
            <title>Inverse of a product</title>
            <p>
                If <m>A, B</m> are <m>n \times n</m> invertible matrices, then their product <m>AB</m> is invertible, and the inverse of <m>AB</m> is the product of the inverses of <m>A</m> and <m>B</m> in reverse order. In other words,
            </p>
            <me>
                (AB)^{-1} = B^{-1} A^{-1}
            </me>
        </theorem>
        <proof>
            <me>
                (AB)(B^{-1} A^{-1}) = A(BB^{-1})A^{-1} = A I_n A^{-1} = AA^{-1} = I_n
            </me>
            Thus, <m>(AB)^{-1} = B^{-1}A^{-1}</m>.
        </proof>

        <p>
            Intuitively, considering matrices <m>A, B</m> as linear transformations, this statement follow from the fact that the inverse of a composition of functions is the composition of the inverse functions in reverse order, or <m>(f \circ g)^{-1} = g^{-1} \circ f^{-1}</m>.
        </p>
        <p>
            This property can be generalized,
        </p>
        <theorem>
            <title>Generalized inverse of a product</title>
            <p>
                If <m>A_1, A_2, \dots, A_n</m> are invertible <m>n \times n</m> matrices, then their product <m>A_1 A_2 \cdots A_n</m> is invertible, and the inverse is the product of their inverses in reverse order,
            </p>
            <me>
                (A_1 \cdots A_n)^{-1} = A_n^{-1} \cdots A_1^{-1}
            </me>
        </theorem>

        <proof>
            <p>
                Intuitively, the inverses will <q>cancel out</q> from inside out,
            </p>
            <md>
                <mrow>\amp (A_1 A_2 \cdots A_{n-1} A_n)(A_n^{-1} A_{n-1}^{-1} \cdots A_2^{-1} A_1^{-1})</mrow>
                <mrow>\amp = A_1 A_2 \cdots A_{n-1} (A_n A_n^{-1}) A_{n-1}^{-1} \cdots A_2^{-1} A_1^{-1}</mrow>
                <mrow>\amp = A_1 A_2 \cdots A_{n-2} (A_{n-1} A_{n-1}^{-1}) A_{n-2}^{-1} \cdots A_2^{-1} A_1^{-1}</mrow>
                <mrow>\amp = \dots</mrow>
                <mrow>\amp = A_1 (A_2 A_2^{-1}) A_1^{-1}</mrow>
                <mrow>\amp = A_1 A_1^{-1}</mrow>
                <mrow>\amp = I_n</mrow>
            </md>
            <p>
                A more precise argument uses mathematical induction.
            </p>
        </proof>
    </subsection>

    <subsection>
        <title>Inverse of a <m>3 \times 3</m> Matrix</title>
        <p>
            The explicit formula for a <m>3 \times 3</m> matrix is more difficult to determine. Consider a <m>3 \times 3</m> matrix <m>A</m>,
        </p>
        <me>
            A = \begin{bmatrix} a_{11} \amp a_{12} \amp a_{13} \\ a_{21} \amp a_{22} \amp a_{23} \\ a_{31} \amp a_{32} \amp a_{33} \end{bmatrix}
        </me>
        <p>
            Using the process of row reduction on a <m>3 \times 3</m> matrix, the formula turns out to be,
        </p>
        <example>
            <p>Derivation (FINISH).</p>
        </example>
        <me>
            A^{-1} = \frac{1}{a_{11} a_{22} a_{33} - a_{11} a_{32} a_{23} + a_{12} a_{23} a_{32} - a_{12} a_{21} a_{33} + a_{13} a_{21} a_{32} - a_{13} a_{22} a_{31}} \begin{bmatrix} a_{22} a_{33} - a_{23} a_{32} \amp -(a_{12} a_{33} - a_{32} a_{13}) \amp a_{12} a_{23} - a_{22} a_{13} \\ -(a_{21} a_{33} - a_{23} a_{31}) \amp a_{11} a_{33} - a_{13} a_{21} \amp -(a_{11} a_{23} - a_{13} a_{21}) \\ a_{21} a_{32} - a_{22} a_{31} \amp -(a_{11} a_{32} - a_{12} a_{31}) \amp a_{11} a_{22} - a_{12} a_{21} \end{bmatrix}
        </me>
        <p>
            provided that <m>Delta = a_{11} a_{22} a_{33} - a_{11} a_{32} a_{23} + a_{12} a_{23} a_{32} - a_{12} a_{21} a_{33} + a_{13} a_{21} a_{32} - a_{13} a_{22} a_{31}</m>. With this notation, the formula simplifies to,
        </p>
        <me>
            \boxed{A^{-1} = \frac{1}{\Delta} \begin{bmatrix} a_{22} a_{33} - a_{23} a_{32} \amp -(a_{12} a_{33} - a_{32} a_{13}) \amp a_{12} a_{23} - a_{22} a_{13} \\ -(a_{21} a_{33} - a_{23} a_{31}) \amp a_{11} a_{33} - a_{13} a_{21} \amp -(a_{11} a_{23} - a_{13} a_{21}) \\ a_{21} a_{32} - a_{22} a_{31} \amp -(a_{11} a_{32} - a_{12} a_{31}) \amp a_{11} a_{22} - a_{12} a_{21} \end{bmatrix}}
        </me>
        <p>
            provided that <m>\Delta \neq 0</m>. Even still, this explicit formula is not to be memorized. There are many patterns in the entries of this matrix, but the full insight can only be understood after consider the later topic of determinants.
        </p>
    </subsection>
</section>
