<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="section-MM">
    <title>Matrix Multiplication</title>

    <introduction>
        <p>
            Addition and scalar multiplication of matrices is defined in the intuitively natural way, entry-wise. However, there is another operation which is not defined in this natural way. That is, multiplying two matrices, or <i>matrix multiplication</i>, is not computed entry-wise. This will be a generalization of the matrix-vector product.
        </p>
        <p>
            Consider a matrix <m>B</m>. If <m>B</m> is multipled to a vector <m>\vec{x}</m>, this transforms <m>\vec{x}</m> into the vector <m>B\vec{x}</m>. Then, if this vector is then multiplied by another matrix <m>A</m>, the resulting vector is <m>A(B\vec{x})</m>.
        </p>
    
        <p>
            The vector <m>A(B\vec{x})</m> is produced from <m>\vec{x}</m> by a <i>composition</i> of transformations. Then, we define <m>AB</m> to be a matrix which represents this single composite transformation produced by applying <m>B</m> and then <m>A</m>.
        </p>
        <p>
            Let <m>A</m> be an <m>m \times n</m> matrix, and <m>B</m> be an <m>n \times p</m> matrix. Let the columns of <m>B</m> be <m>\vec{b}_1, \dots, \vec{b}_p</m>, and let <m>\vec{x} = (x_1, \dots, x_p)</m> be a vector in <m>\mathbb{R}^p</m>. Then, first consider the matrix-vector product <m>B\vec{x}</m>,
        </p>
        <me>
            B\vec{x} = x_1 \vec{b}_1 + \dots + x_p \vec{b}_p
        </me>
        Then, consider <m>A(B\vec{x})</m>,
        <me>
            A(B\vec{x}) = A\brac{x_1 \vec{b}_1 + \dots + x_p \vec{b}_p}
        </me>
        Here, <m>A</m> is being applied to a linear combination of <m>\vec{b}_1, \dots, \vec{b}_p</m>, with weights <m>x_1, \dots, x_p</m>. So, by linearity, this is equivalent to,
        <me>
            A(B\vec{x}) = x_1 A\vec{b}_1 + \dots + x_p A \vec{b}_p
        </me>
        <p>
            In other words, the vector <m>A(B\vec{x})</m> is the linear combination of <m>A\vec{b}_1, \dots, A\vec{b}_p</m>, with the entries of <m>\vec{x}</m> as weights. In matrix notation,
        </p>
        <me>
            A(B\vec{x}) = \begin{bmatrix} A \vec{b}_1 \amp \cdots \amp A \vec{b}_p \end{bmatrix} \vec{x}
        </me>
        <p>
            Thus, the matrix <m>\begin{bmatrix} A \vec{b}_1 \amp \cdots \amp A \vec{b}_p \end{bmatrix}</m> is precisely the matrix which transforms <m>\vec{x}</m> into <m>A(B\vec{x})</m>, which we want to denote by <m>AB</m>.
        </p>
    </introduction>

    <subsection>
        <title>Matrix Multiplication</title>

        <definition>
            Let <m>A</m> be an <m>m \times n</m> matrix, <m>B</m> be an <m>n \times p</m> matrix with columns <m>\vec{b}_1, \dots, \vec{b}_p</m>. Then, the <term>product</term> <m>AB</m> is the <m>m \times p</m> matrix with columns <m>A\vec{b}_1, \dots, A\vec{b}_p</m>, or,
            <me>
                \boxed{AB = A \begin{bmatrix} \vec{b}_1 \amp \cdots \amp \vec{b}_p \end{bmatrix} = \begin{bmatrix} A \vec{b}_1 \amp \cdots \amp A \vec{b}_p \end{bmatrix}}
            </me>
        </definition>

        <p>
            This definition for matrix multiplication makes the equation <m>A(B\vec{x}) = (AB)\vec{x}</m> true. That is, <m>AB</m> is the single transformation that is equvialent to applying <m>B</m> first, and then <m>A</m>. In this way, multiplication of matrices corresponds to composition of linear transformations.
        </p>
        <p>

        </p>
    </subsection>

    <subsection>
        <title>Matrix Multiplication as a Dot Product</title>
    </subsection>

    <subsection>
        <title>Properties of Matrix Multiplication</title>

        <p>
            Matrix multiplication shares many properties of real number multiplication, assuming the matrices have the appropriate dimensions.
        </p>

        <theorem>
            <p>
                Let <m>A</m> be an <m>m \times n</m> matrix, <m>B</m> and <m>C</m> be <m>n \times p</m> matrices, <m>D</m> and <m>E</m> be <m>q \times m</m> matrices, <m>a \in \mathbb{R}</m> be a scalar. Then,
            </p>
            <ol>
                <li><em>Multiplication distributes over addition</em>.
                <me>
                    A(B + C) = AB + AC \qquad \text{and} \qquad (D + E)A = DA + EA
                </me></li>
                <li><em>Associative with scalar multiplication</em>.
                <me>
                    a(AB) = (aA)B = A(aB)
                </me></li>
                <li><em>Associative with matrix multiplication</em>.
                <me>
                    A(BC) = (AB)C
                </me></li>
            </ol>
        </theorem>

        <p>
            These properties intuitively means that parentheses surrounding matrix expressions generally work the same as for real numbers.
        </p>

        <proof>
            <ol>
                <li><em>Proof of 1. entry-wise</em>.
                <md>
                    <mrow>(A(B + C))_{ij} \amp = \sum_{k=1}^n A_{ik}(B + C)_{kj} \amp\amp \text{definition of matrix multiplication}</mrow>
                    <mrow>\amp = \sum_{k=1}^n A_{ik}(B_{kj} + C_{kj}) \amp\amp \text{definition of matrix addition}</mrow>
                    <mrow>\amp = \sum_{k=1}^n (A_{ik} B_{kj} + A_{ik} C_{kj})</mrow>
                    <mrow>\amp = \sum_{k=1}^n A_{ik}B_{kj} + \sum_{k=1}^n A_{ik}C_{kj}</mrow>
                    <mrow>\amp = (AB)_{ij} + (AC)_{ij} \amp\amp \text{definition of matrix multiplication}</mrow>
                    <mrow>\amp = (AB + AC)_{ij} \amp\amp \text{definition of matrix addition}</mrow>
                </md></li>
                <li>
                    <em>Proof of 3. for <m>2 \times 2</m> matrices.</em> Let <m>A = \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}, B = \begin{bmatrix} e \amp f \\ g \amp h \end{bmatrix}, C = \begin{bmatrix} i \amp j \\ k \amp l \end{bmatrix}</m>. Then,
                    <md>
                        <mrow>A(BC) = \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix} \brac{\begin{bmatrix} e \amp f \\ g \amp h \end{bmatrix} \begin{bmatrix} i \amp j \\ k \amp l \end{bmatrix}} \amp = \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix} \begin{bmatrix} ei + fk \amp ej + fl \\ gi + hk \amp gj + hl \end{bmatrix}</mrow>
                        <mrow>\amp = \begin{bmatrix} a(ei + fk) + b(gi + hk) \amp a(ej + fl) + b(gj + hl) \\ c(ei + fk) + d(gi + hk) \amp c(ej + fl) + d(gj + hl) \end{bmatrix}</mrow>
                    </md>
                    Also,
                    <md>
                        <mrow>(AB)C = \brac{\begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix} \begin{bmatrix} e \amp f \\ g \amp h \end{bmatrix}} \begin{bmatrix} i \amp j \\ k \amp l \end{bmatrix} \amp = \begin{bmatrix} ae + bg \amp af + bh \\ ce + dg \amp cf + dh \end{bmatrix} \begin{bmatrix} i \amp j \\ k \amp l \end{bmatrix}</mrow>
                        <mrow>\amp = \begin{bmatrix} (ae + bg)i + (af + bh)k \amp (ae + bg)j + (af + bh)l \\ (ce + dg)i + (cf + dh)k \amp (ce + dg)j + (cf + dh)l </mrow>
                    </md>
                    Comparing these expressions entry-by-entry shows that they are equal.
                </li>
            </ol>
        </proof>
    </subsection>

    <subsection>
        <title>Matrix Multiplication is Not Commutative</title>

        <p>
            While matrix multiplication shares many properties of real number multiplication, it differs by an important property. That is, matrix multiplication is not commutative, in that in general, <m>AB \neq BA</m>. Viewing <m>A</m> and <m>B</m> as transformations, it clear that applying <m>B</m> first, and then <m>A</m>, will not be the same as applying <m>A</m> first, and then <m>B</m>. You may recall the more specific property holds for functions, that is, for real-valued functions <m>f, g</m>, <m>f(g(x)) \neq g(f(x))</m>. That is, the non-commutativity of matrix multiplication is a consequence of the more general fact that transformation composition is not commutative.
        </p>
        <p>
            In fact, viewing <m>A</m> and <m>B</m> as matrices, suppose say <m>A</m> is an <m>m \times n</m> matrix, and <m>B</m> is an <m>n \times p</m> matrix. Then, the product <m>AB</m> is defined. However, the product <m>BA</m> is not even defined in general, unless <m>m = p</m>. Further, even if <m>m = p</m>, the product <m>AB</m> will have dimensions <m>m \times m</m>, while the product <m>BA</m> will have dimensions <m>n \times n</m>, and so <m>AB</m> and <m>BA</m> cannot even possibly be the same matrix, unless <m>m = n</m>. In this special case, where <m>m = n</m> and <m>m = p</m>, <m>A</m> and <m>B</m> are both square matrices, say <m>n \times n</m>. Even in this particular case, the entries of <m>A</m> and <m>B</m> are not all the same in general, that is, <m>AB \neq BA</m>.
        </p>
        <p>
            More concretely, <m>AB</m> is a linear combination of the columns of <m>B</m>, whereas <m>BA</m> is a linear combination of the columns of <m>A</m>, and there is no reason to expect these to be equal.
        </p>
        <p>
            Then, in the product <m>AB</m>, we sometimes say <m>A</m> is <term>right-multiplied</term> by <m>B</m>, or <m>B</m> is <term>left-multiplied</term> by <m>A</m>.
        </p>
    </subsection>

    <subsection>
        <title>The Identity Matrix</title>

        <p>
            Recall that for real numbers, multiplying a number by 1 results in the number being unchanged. In other words, 1 is the <i>multiplicative identity</i> for multiplication of real numbers, <m>a \cdot 1 = a</m> for all <m>a \in \mathbb{R}</m>. Similarly, we can define a matrix which, when multiplied by a matrix, leaves it unchanged, called an \textit{identity matrix}.
        </p>
    </subsection>


</section>
