\documentclass[letterpaper,12pt]{article}
\input{preamble}

\chead{Inverse of a Matrix}

\begin{document}

The \textit{inverse} of a matrix is the analogue of the reciprocal, or multiplicative inverse, of a non-zero number. For example, $3^{-1} = \frac{1}{3}$ because $3 \cdot \frac{1}{3} = 1$ and $\frac{1}{3} \cdot 3 = 1$. For numbers, both equalities hold simultaneously because multiplication is commutative. For matrices, recall that matrix multiplication is not commutative, so both equalities must be true. Further, for both sides of the product to be defined, the matrices involved must be square.

\section*{Inverse of a Matrix}
\begin{definition}
An $n \times n$ matrix $A$ is \textbf{invertible} if there exists an $n \times n$ matrix $B$ such that $AB = BA = I_n$. Then, $B$ is the \textbf{inverse} of $A$.
\end{definition}

The inverse of a matrix $A$ is unique, so \textit{the} inverse of $A$ is well-defined. This is because if $C$ is another inverse of $A$ (so that $AC = CA = I_n$), then
\begin{equation*}
    B = BI_n = B(AC) = (BA)C = I_n C = C
\end{equation*}
and so $B = C$. Then, the inverse of $A$ is denoted by $A^{-1}$, and has the property that,
\begin{equation*}
    AA^{-1} = A^{-1} A = I_n
\end{equation*}

\begin{itemize}
    \item A matrix which is not invertible is sometimes called \textbf{singular}, and a matrix which is invertible is called \textbf{non-singular}.
\end{itemize}

For a square matrix $A$, $AB = I_n$ if and only if $BA = I_n$, so it is sufficient to compute only one of the two products $AB$ or $BA$ to determine whether $B$ is the inverse of $A$.
\\ \\ Considering a matrix $A$ as a linear transformation, $A^{-1}$ is the inverse linear transformation, which has the property that its composition with $A$ is equivalent to the identity map.



\section*{Inverse of a $2 \times 2$ Matrix}
For a $2 \times 2$ matrix, there is a simple test for invertibility, and an explicit formula for the inverse.

\begin{theorem}
Let $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ be a $2 \times 2$ matrix. If $ad - bc \neq 0$, then $A$ is invertible, and
\begin{equation*}
    \boxed{A^{-1} = \frac{1}{ad - bc} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}}
\end{equation*}
\end{theorem}

If $ad - bc = 0$, the matrix $A$ does not have an inverse. Of course, a ``proof" that this formula is correct can simply involve multiplying to verify that $AA^{-1} = I_2$. The method of actually determining the correct formula for $A^{-1}$ from $A$ is more complicated.

\begin{proof}

\begin{align*}
    AA^{-1} & = \frac{1}{ad - bc} \begin{bmatrix} a & b \\ c & d \end{bmatrix} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix} \\
    & = \frac{1}{ad - bc} \begin{bmatrix} ad - bc & 0 \\ 0 & ad - bc \end{bmatrix} \\
    & = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} = I_2
\end{align*}
\end{proof}


\section*{Solving Systems with Inverse Matrices}
\begin{theorem}
If $A$ is an invertible $n \times n$ matrix, then for every $\vec{b} \in \mathbb{R}^n$, the equation $A\vec{x} = \vec{b}$ has a unique solution, given by $\vec{x} = A^{-1} \vec{b}$.
\end{theorem}

Intuitively, this comes from multiplying both sides of the equation $A\vec{x} = \vec{b}$ on the left by $A^{-1}$, to get $A^{-1}A \vec{x} = A^{-1} \vec{b}$.  Then, $A^{-1} A = I_n$ and $I_n \vec{x} = \vec{x}$, so the equation becomes $\vec{x} = A^{-1} \vec{b}$. The uniqueness follows from the fact that matrix inverses are unique.

\begin{proof}
First, indeed $\vec{x} = A^{-1} \vec{b}$ is a solution, as
\begin{align*}
    A(A^{-1} \vec{b}) & = (AA^{-1}) \vec{b} = I_n \vec{b} = \vec{b}
\end{align*}
so it indeed satisfies the equation. For uniqueness, let $\vec{u}$ be any solution, so that $A\vec{u} = \vec{b}$. Then, multiplying on the left by $A^{-1}$, $A^{-1} A \vec{u} = A^{-1} \vec{b}$, or $I_n \vec{u} = A^{-1} \vec{b}$, and so $\vec{u} = A^{-1} \vec{b}$.
\end{proof}

The formula $\vec{x} = A^{-1} \vec{b}$ is rarely used in practice to solve a system of equations, because row reduction is almost always faster, especially for large systems, for which computing inverses is very time-consuming. A possible exception is for systems of two equations in two unknowns.


\section*{Properties of Inverses}
\begin{theorem}
\textbf{Inverse of an inverse}. If $A$ is invertible, then $A^{-1}$ is invertible, and
\begin{equation*}
    \boxed{(A^{-1})^{-1} = A}
\end{equation*}
In other words, the inverse of $A^{-1}$ is $A$.
\end{theorem}

\begin{proof}
The inverse of $A^{-1}$ is a matrix $B$ such that $A^{-1} B = B A^{-1} = I_n$. Clearly, $A$ satisfies this property, as $A^{-1} A = A A^{-1} = I_n$.
\end{proof}

\begin{theorem}
\textbf{Inverse of a product}. If $A, B$ are $n \times n$ invertible matrices, then $AB$ is invertible, and the inverse of $AB$ is the product of the inverses of $A$ and $B$ in reverse order, or
\begin{equation*}
    \boxed{(AB)^{-1} = B^{-1} A^{-1}}
\end{equation*}
\end{theorem}

\begin{proof}
\begin{align*}
    (AB)(B^{-1}A^{-1}) & = A(BB^{-1})A^{-1} = AI_n A^{-1} = AA^{-1} = I_n
\end{align*}
Thus, $(AB)^{-1} = B^{-1} A^{-1}$.
\end{proof}

Intuitively, considering matrices $A, B$ as linear transformations, this statement follow from the fact that the inverse of a composition of functions is the composition of the inverse functions in reverse order, or $(f \circ g)^{-1} = g^{-1} \circ f^{-1}$.
\\ \\ Of course, this can be generalized,
\begin{theorem}
\textbf{Generalized inverse of a product}. sIf $A_1, \dots, A_n$ are invertible, then their product $A_1 \cdots A_n$ is invertible, and the inverse is the product of their inverses in reverse order,
\begin{equation*}
    (A_1 \cdots A_n)^{-1} = A_n^{-1} \cdots A_1^{-1}
\end{equation*}
\end{theorem}

\begin{theorem}
\textbf{Inverse of a transpose}. If $A$ is invertible, then $A^T$ is also invertible, and the inverse of $A^T$ is the transpose of $A^{-1}$, or
\begin{equation*}
    (A^T)^{-1} = (A^{-1})^T
\end{equation*}
\end{theorem}

\begin{proof}
\begin{align*}
    A^T (A^{-1})^T & = (AA^{-1})^T && \text{transpose of a product} \\
    & = (I_n)^T \\
    & = I_n
\end{align*}
\end{proof}

\end{document}