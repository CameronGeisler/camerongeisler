\documentclass[letterpaper,12pt]{article}
\newcommand{\myname}{Cameron Geisler}
\newcommand{\mynumber}{90856741}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[paper=letterpaper,left=25mm,right=25mm,top=3cm,bottom=25mm]{geometry}
\usepackage{fancyhdr}
\usepackage{amsthm}
\usepackage{siunitx}
\pagestyle{fancy}

\lhead{Math 223}
\chead{Lecture 33, Quadratic Form, Sylvester Inertia theorem}
\rhead{\myname \\ \mynumber}
\lfoot{\myname}
\cfoot{Page \thepage}
\rfoot{\mynumber}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0.4pt}
\renewcommand\labelitemii{\textbullet} %changes 2nd level bullet to bullet

\setlength{\parindent}{0pt}
\theoremstyle{definition}
\newtheorem*{result}{Result}
\newtheorem*{definition}{Definition}
\newtheorem*{theorem}{Theorem}
\newtheorem*{example}{Example}
\newtheorem*{corollary}{Corollary}
\newtheorem*{lemma}{Lemma}
\usepackage{enumerate}

%% Math
\newcommand{\abs}[1]{\left\lvert #1 \right\rvert}
\newcommand{\set}[1]{\left\{ #1 \right\}}

%% Linear algebra
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\nullity}{nullity}
\DeclareMathOperator{\Image}{Im}
\newcommand{\Span}[1]{\text{Span}\left(#1 \right)}
\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\colrk}{colrk}
\DeclareMathOperator{\rowrk}{rowrk}
\DeclareMathOperator{\Row}{Row}
\DeclareMathOperator{\Col}{Col}
\DeclareMathOperator{\matref}{ref}
\DeclareMathOperator{\matrref}{rref}
\DeclareMathOperator{\sol}{Sol}
\newcommand{\inp}[2]{\left< #1, #2 \right>}
\newcommand{\norm}[1]{\| #1 \|}

\begin{document}

\begin{definition}
Let $V$ be a vector space over $\mathbb{F}$, $\operatorname{char}(\mathbb{F}) \neq 2$. Then, $q: V \rightarrow \mathbb{F}$ is a \textbf{quadratic form} if and only if there exists a symmetric billinear form $\beta: V \times V \rightarrow \mathbb{F}$ such that $q(v) = \beta(v,v)$.
\begin{itemize}
    \item $\beta$ is determined by $q$
    \item If $\mathcal{B} = (v_1, \dots, v_n)$ is a basis of $V$, then the matrix of $q$ with respect to $\mathcal{B}$, is $[q]_{\mathcal{B}} = (\beta(v_i, v_j))_{ij}$
    \item The kernel of $q$, $\ker{q}$, is defined as
    \begin{align*}
        \ker{q} = \ker{\left(V \longrightarrow \operatorname{Hom}(V, \mathbb{F}), v \longmapsto \beta(v, \cdot) \right)}
    \end{align*}
    \item $q$ is \textbf{non-degenerate} if and only if $\ker{q} = \set{0}$, as if $\beta{v,x} = 0$ for all $x$, then $v = 0$.
    \item $\rk{q} = \dim{V} - \dim{(\ker{q})}$
\end{itemize}
\end{definition}


\begin{theorem}
The rank of $q$, $\rk{q}$, is the maximal dimension of a subspace $U \subset V$ such that $q \mid U$ ($q$ restricted to $U$) is non-degenerate.
\end{theorem}
\begin{proof}
Choose a basis $\mathcal{B} = (v_1, \dots, v_n)$ of $V$ such that $[q]_{\mathcal{B}}$ is diagonal. Then, assume for some $1 \leq l \leq n$, $q(v_1) \neq 0, \dots, q(v_l) \neq 0$, and $q(v_{l+1}) = \dots q(v_n) = 0$. Then, we want to show that $q \mid \Span{(v_1, \dots, v_l)}$ is non-degenerate.
\\ \\ Let $\beta \left( \sum_{i \leq l} \alpha_i v_i \right) = 0$. Then, for all $j \leq l$,
\begin{align*}
    0 & = \beta \left( \sum_{i \leq l} \alpha_i v_i, v_j \right) \\
    & = \sum_{i \leq l} \alpha_i \beta(v_i, v_j) \\
    & = \alpha_j q(v_j) \\
\end{align*}
Thus, $\alpha_1, \dots, \alpha_l = 0$. Then, we want to show that $\ker{q} = \Span{(v_{l+1}, \dots, v_n)}$. Let $\sum_{i=l}^n \alpha_i v_i \in \ker{q}$. Then,
\begin{align*}
    0 & = \beta \left( \sum_{i} \alpha_i v_i, \cdot \right) \\
\end{align*}
Thus, $\rk{q} = \dim{V} - \dim{\ker{q}} = n - (n-l) = l$. Thus, if $U \subset V$ is such that $\dim{U} > l$, then $U \cap \ker{q} \neq \set{0}$. Therefore, $q \mid U$ has a non-trivial kernel so $q \mid U$ is non-degenerate.
\end{proof}



\section*{Quadratic Forms over $\mathbb{C}$}
$\mathbb{F} = \mathbb{C}$, $V$ is finite dimensional, $q: V \rightarrow \mathbb{C}$, $\rk{q} = k$.
\\ \\ Since $\mathbb{F} = \mathbb{C}$, $\forall x \in \mathbb{C}$, $\exists y \in \mathbb{C}$ such that $y^2 = x$, or $y = \sqrt{x}$.
\\ \\ Arrange an ``orthonormal basis" such that $q(v_1), \dots, q(v_k) \neq 0$ and $q(v_{k+1}) = \dots = q(v_n) = 0$. Then, for all $i \leq l$ replace $v_i$ with $v_i / \sqrt{q(v_i)}$. Then,
\begin{align*}
    q(\dfrac{v_i}{\sqrt{q(v_i)}}) & = \betaf{\dfrac{v_i}{\sqrt{q(v_i)}}}{\dfrac{v_i}{\sqrt{q(v_i)}}} \\
    & = \dfrac{q(v_i)}{q(v_i)} \\
    & = 1
\end{align*}
Then, $[q]_{\mathcal{B}} = \begin{pmatrix} I_k & 0 \\ 0 & 0 \end{pmatrix}$. See OneNote.

\section*{Sylvester Inertia theorem}
\begin{theorem}
Let $\mathbb{F} = \mathbb{R}$, $\dim{V} = n < \infty$, $q: V \rightarrow \mathbb{R}$ be a quadratic form. Then, there exists a basis (a Sylvester basis) $\mathbb{B}$ of $V$ such that
\begin{align*}
    [q]_{\mathcal{B}} = \begin{pmatrix} I_r & 0 & 0 \\ 0 & I_s & 0 \\ 0 & 0 & 0 \end{pmatrix}
\end{align*}
$r+s = \rk{q}$, $r$ is the maximal dimension of a subspace $U \subset V$ such that $q \mid U$ is positive definite. $r-s$ is the signture of $q$
\end{theorem}
\begin{proof}
Start with a basis $\mathcal{B} = (v_1, \dots, v_n)$ such that $[q]_{B}$ is diagonal. For each $v_i$, if $q(v_i) \neq 0$, replace $v_i$ with $v_i/\sqrt{\abs{q(v_i)}}$. Then,
\begin{align*}
    q \left( \dfrac{v_i}{\sqrt{\abs{q(v_i)}}} \right) & = \dfrac{q(v_i)}{\abs{q(v_i)}} = \pm 1
\end{align*}
Then, rearrange to get a Sylvester basis, and $r+s = \rk{q}$.
\\ \\ Next, prove that $r$ is the max dimension of a positive definite subspace. $q(\Span{(v_1, \dots, v_n)}$ is positive definite because the matrix of $q$ is $I_r$. $q$ turns into the standard inner product which is positive definite. If $U \subset V$ and $\dim{U} > r$, then $U \cap \Span{(v_{r+1}, \dots, v_n)} \neq \set{0}$. Prove that
\begin{align*}
    q \left( \sum_{i > r} \alpha_i v_i \right) = \sum_{i > r} \alpha_{i}^{2} q(v_i) \leq 0
\end{align*}
so $q \mid U$ is not positive definite.
\end{proof}

\section*{Quadratic Forms over Euclidian Vector Space}
Let $V$ be a finite-dimensional Euclidian vector space, $q: V \rightarrow \mathbb{R}$ be a quadratic form. We only want to allow orthonormal bases in $V$ (in Cartesian coordinate systems). $q \iff \beta$. Then, there exists a unique self-adjoint map $f: V \rightarrow V$ such that $\beta(x,y) = \inp{x}{f(y)}$
\\ \\ $f$ is self-adjoint, so there exists an orthonormal basis $\mathcal{B}$ of eigenvectors for $f$. Then, $[q]_{\mathcal{B}} = \begin{pmatrix} \lambda_i & 0 \\ 0 & \lambda_n \end{pmatrix}$

\section*{How to Detemine a Sylvester Basis}
\begin{enumerate}
    \item Determine $v_1$ such that $q(v_1) \neq 0$
    \item Determine $v_2$ such that $q(v_2) \neq 0$ and $\beta(v_1, v_2) = 0$
    \item Determine $v_3$ such that $q(v_3) \neq 0$, $\beta(v_1, v_3) = \beta(v_2, v_3) = 0$
    \item ...
    \item Then, ``normalize" the vectors by replacing $v_i \rightarrow v_i / \sqrt{\abs{q(v_i)}}$
\end{enumerate}
Alternatively, use a modified Gram-Schmidt to convert a given basis into a Sylvester basis.
\\ \\ Alternative, use a method with elementary symmetric operations (from the textbook Janich)


\section*{Next Lecture}
\section*{Quadratic Forms in Euclidian Vectors Spaces}
Let $V$ be a Euclidian vector space, $q: V \rightarrow \mathbb{R}$ be a quadtratic form corresponding to $\beta: V \times V \rightarrow \mathbb{R}$. Then, there exists a unique self-adjoint operator $f: V \rightarrow V$ such that $\beta(x,y) = \inp{x}{f(y)}$
\\ \\ If $\mathcal{B} = (u_1, \dots, u_n)$ is an orthonormal basis of $V$, then $[q]_{\mathcal{B}} = [f]_{\mathcal{B}}$, as $\beta(u_i, u_j) = \inp{u_i}{f(u_j)}$
\\ \\ See OneNote.
\\ \\ Let $\mathcal{B} = (u_1, \dots, u_n)$ be an orthonormal basis of eigenvectors for $f$, where $\lambda_i$ is the eigenvalue associated with $u_i$. Then,
\begin{align*}
    \beta(u_i, u_j) & = \inp{u_i}{f(u_j)} \\
    & = \lambda_j \inp{u_i}{u_j} \\
    & = \lambda \delta_{ij}
\end{align*}

\section*{New Interpretation of the Spectral Theorem, the Principal Axes Theorem}
\begin{theorem}
Let $V$ be a Euclidian vector space, $q: V \rightarrow \mathbb{R}$ be a quadratic form. Then, there exists an orthonormal basis $\mathcal{B} = (u_1, \dots, u_n)$ of $V$ such that $[q]_{\mathcal{B}} = \operatorname{diag}(\lambda_1, \dots, \lambda_n)$.
\end{theorem}
\\ \\ In the coordinates given by $\mathcal{B}$,
\begin{align*}
    q \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} & = \lambda_1 x_{1}^{2} + \dots + \lambda_n x_{n}^{2}
\end{align*}
The ``conic section" $q(x) = 1$ or $\lambda_1 x_{1}^{2} + \dots + \lambda_n x_{n}^{2} = 1$.
\\ \\ $\Span{u_i}$ are the coordinate axes, the principal axes of $q(x) = 1$. If all of $\lambda_i > 0$, then the intercepts of $q(x) = 1$ with $\Span{u_i}$ is given by $\lambda_i x_{i}^{2} = 1$, or $x_i = \pm 1/\sqrt{\lambda_i}$. $1/\sqrt{\lambda_i}$ is the $i$th semi-axis. If $\lambda_i < 0$, then there are no intercepts with the $i$th principal axis.

\begin{example}
Let $V = (\mathbb{R}^2, \operatorname{std})$,
\begin{equation*}
    q \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} = 2x_1^2 + 6x_1 x_2 + 2x_2^2
\end{equation*}
\begin{equation*}
    [q] = \begin{pmatrix} 2 & 3 \\ 3 & 2 \end{pmatrix}
\end{equation*}
\begin{align*}
    \beta \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \begin{pmatrix} y_1 \\ y_1 \end{pmatrix} & = \begin{pmatrix} x_1 & x_2 \end{pmatrix} \begin{pmatrix} 2 & 3 \\ 3 & 2 \end{pmatrix} \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} \\
    & = 2x_1 y_1 + 3x_1 y_2 + 3x_2 y_1 + 2x_2 y_2
\end{align*}
The principal axes are $\Span{\begin{pmatrix} 1 \\ 1 \end{pmatrix}}$ and $\Span{\begin{pmatrix} 1 \\ -1 \end{pmatrix}}$
\end{example}


\section*{Classification of Matrices}
\begin{enumerate}
    \item Class of quadratic forms over $\mathbb{R}$
    \\ Let $V$ be a vector space over $\mathbb{R}$, $q: V \rightarrow \mathbb{R}$ and $q': V \rightarrow \mathbb{R}$ are \textbf{equivalent} if and only if there exists an automorphism $\Phi: V \rightarrow V$ such that $q' \circ \Phi = q$
\end{enumerate}

\begin{theorem}
$q$ is equivalent to $q'$ if and only if $\rk{q} = \rk{q'}$ and $\sgn{q} = \sgn{q'}$
\end{theorem}
\begin{definition}
Symmetric matrices $A$ and $B$ are \textbf{equivalent} if and only if there exists an invertible matrix $\Phi$ such that $A = \Phi^{t}B \Phi$
\end{definition}
\begin{theorem}
Every symmetric matrix is equivalent to a matrix in Sylvester form.
\end{theorem}

\begin{corollary}
Two symmetric $A$ and $B$ are equivalent if and only if their Sylvester forms are equal.
\end{corollary}

\begin{definition}
$A$, $B \in M(n \times n, \mathbb{F})$ are \textbf{similar}, $A ~ B$ if and only if there exists an invertible matrix $P$ such that $A = PBP^{-1}$
\end{definition}

If $f: V \rightarrow V$ is an endomorphism, $\mathcal{B}$, $\mathcal{C}$ bases of $V$, then $[f]_{B} = [id]_{\mathcal{B}}^{\mathcal{C}} [f]_{\mathcal{C}} [id]_{\mathcal{C}}^{\mathcal{B}}$, so $[f]_{\mathcal{B}} = [f]_{\mathcal{C}}$.
\\ \\ If $[f]_{\mathcal{B}} ~ A$, then there exists a basis $\mathcal{C}$ such that $A = [f]_{\mathcal{C}}$

\begin{corollary}
If $A$ and $B$ are diagonalizable (i.e. $A$ and $B$ are both similar to a diagonalizable matrix), then $A ~ B$ if and only if the eigenvalues and associated multiplicities are equal, if and only if their characteristic polynomials are equal.
\begin{itemize}
    \item In general, if $\mathbb{F} = \mathbb{C}$, then $A ~ B$ if and only if their Jordan canonical forms are equal (up to ordering of Jordan blocks)
\end{itemize}
\end{corollary}

A Jordan block is $\begin{pmatrix} \lambda^1 & 0 \\ 0 & \lambda^1 \end{pmatrix}$
\begin{definition}
$A$, $B \in M(n \times n, \mathbb{R})$ are \textbf{orthogonally similar} if and only if there exists an orthogonal matrix $P$ such that $A = PBP^{-1}$
\end{definition}

Spectral theorem
\begin{theorem}
$A$ is orthogonally similar to $B$ if and only if all eigenvalues are equal ($\lambda_i = \mu_i$)
\end{theorem}

$4$ Fundamental Theorems of Linear Algebra
\begin{enumerate}
    \item Rank Theorem (for general matrices over an arbitrary field). $A$ is row equivalent (or column equivalent) to $B$ if and only if $\rk{A} = \rk{B}$
    \item Sylvester theorem. For $A$, $B$ symmetric over $\mathbb{R}$ (also a version over $\mathbb{C}$), $A$ and $B$ are equivalent if and only if $\rk{A} = \rk{B}$ and $\sgn{A} = \sgn{B}$
    \item For $A$, $B$ square matrices over $\mathcal{C}$, $A ~ B$ if and only if Jordan forms are equal
    \item Spectral theorem / Principal axes theorem, for $A$, $B$ symmetric over $\mathbb{R}$, $A$ is orthogonally similar to $B$ if and only if the eigenvalues are equal.
\end{enumerate}

\end{document}