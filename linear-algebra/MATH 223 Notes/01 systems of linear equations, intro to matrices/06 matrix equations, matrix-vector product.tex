\documentclass[letterpaper,12pt]{article}
\input{preamble}

\chead{Matrix Equations, Matrix-Vector Product}

\begin{document}

Next, we will combine matrices and vectors, in order to write a system of linear equations as an equation relating matrices and vectors, of the form $A\vec{x} = \vec{b}$, where $A$ is the coefficient matrix of the system, $\vec{x}$ is the vector of unknowns, and $\vec{b}$ is the constant vector.

\section*{Product of Matrix with a Vector}
\begin{definition}
Let $A = \begin{bmatrix} a_{ij} \end{bmatrix}$ be an $m \times n$ matrix, $\vec{x} \in \mathbb{R}^n$ be a vector, $\vec{x} = (x_1, \dots, x_n)$. Then, the \textbf{product} of $A$ and $\vec{x}$, $A\vec{x}$, is the vector defined by,
\begin{equation*}
    \boxed{A\vec{x} = \begin{bmatrix} a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n \\ a_{21} x_1 + a_{22} x_2 + \dots + a_{2n} x_n \\ \vdots \\ a_{m1} x_1 + a_{m2} x_2 + \dots + a_{mn} x_n \end{bmatrix} = \begin{bmatrix} \sum_{k=1}^n a_{1k} x_k \\ \sum_{k=1}^n a_{2k} x_k \\ \vdots \\ \sum_{k=1}^n a_{mk} x_k \end{bmatrix}}
\end{equation*}
\end{definition}

In general, to determine the $i$th entry of $A\vec{x}$, this involves computing the ``product" of the $i$th row of $A$, and $\vec{x}$, as
\begin{equation*}
    \begin{bmatrix} a_{i1} & a_{i2} & \dots & a_{in} \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = \begin{bmatrix} a_{i1} x_1 + a_{i2} x_2 + \dots + a_{in} x_n \end{bmatrix}
\end{equation*}
This operation, of multiplying two vectors of matching lengths, in particular a row vector and a column vector, is called the \textbf{inner product}, or \textbf{scalar product}, or \textbf{dot product}, of the two vectors. More generally,
\begin{equation*}
    \boxed{\begin{bmatrix} a_1 & a_2 & \dots & a_n \end{bmatrix} \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix} = a_1 b_1 + a_2 b_2 + \dots + a_n b_n}
\end{equation*}

Multiplication of a matrix and a vector is precisely defined as to align with systems of equations. In particular, the system of equations,
\begin{align*}
    a_{11}x_1 + a_{12}x_2 + \dots + a_{1n}x_n & = b_1 \\
    a_{21}x_1 + a_{22}x_2 + \dots + a_{2n}x_n & = b_2 \\
    \vdots \qquad \qquad \qquad \qquad \qquad & \vdots \\
    a_{m1}x_1 + a_{m2}x_2 + \dots + a_{mn}x_n & = b_n
\end{align*}
can be written as $A\vec{x} = \vec{b}$, where
\begin{equation*}
    \underbrace{A = \begin{bmatrix}
    a_{11} & a_{12} & \dots & a_{1n} \\
    a_{21} & a_{22} & \dots & a_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1} & a_{n2} & \dots & a_{mn}
    \end{bmatrix}}_{\text{matrix of coefficients}}
    \qquad
    \underbrace{\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix}}_{\text{vector of unknowns}} \qquad \underbrace{\vec{b} = \begin{bmatrix} b_1 \\ b_2 \\ \vdots \\ b_n \end{bmatrix}}_{\text{constant vector}}
\end{equation*}
    
\section*{Linear Combination Interpretation}
From the previous definition of a matrix-vector product, we can expand out the product as,
\begin{align*}
    A\vec{x} = \begin{bmatrix} a_{11} x_1 + a_{12} x_2 + \dots + a_{1n} x_n \\ a_{21} x_1 + a_{22} x_2 + \dots + x_{2n} x_n \\ \vdots \\ a_{m1} x_1 + a_{m2} x_2 + \dots + a_{mn} x_n \end{bmatrix} & = \begin{bmatrix} a_{11} x_1 \\ a_{21} x_1 \\ \vdots \\ a_{m1} x_1 \end{bmatrix} + \begin{bmatrix} a_{12} x_2 \\ a_{22} x_2 \\ \vdots \\ a_{m2} x_2 \end{bmatrix} + \dots + \begin{bmatrix} a_{1n} x_n \\ a_{2n} x_n \\ \vdots \\ a_{mn} x_n \end{bmatrix} \\
    & = x_1 \begin{bmatrix} a_{11} \\ a_{21} \\ \vdots \\ a_{m1} \end{bmatrix} + x_2 \begin{bmatrix} a_{12} \\ a_{22} \\ \vdots \\ a_{m2} \end{bmatrix} + \dots + x_n \begin{bmatrix} a_{1n} \\ a_{2n} \\ \vdots \\ a_{mn} x_n \end{bmatrix}
\end{align*}
In this way, the product $A\vec{x}$ is precisely a linear combination of the columns of $A$, with weights given by the entries of $\vec{x}$. In summary,

\begin{theorem}
The product $A\vec{x}$ is the linear combination of the columns of $A$ with the corresponding entries of $\vec{x}$ as weights. More precisely, if $A = \begin{bmatrix} \vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \end{bmatrix}$ and $\vec{x} = (x_1, \dots, x_n)$, then
\begin{equation*}
    \boxed{A \vec{x} = \begin{bmatrix} \vec{a}_1 & \vec{a}_2 & \cdots & \vec{a}_n \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{bmatrix} = x_1 \vec{a}_1 + \dots + x_n \vec{a}_n}
\end{equation*}
\end{theorem}

\section*{Matrix Equations as Linear Systems}
\begin{theorem}
Let $A$ be an $m \times n$ matrix with columns $\vec{a}_1, \dots, \vec{a}_n$, and let $\vec{b} \in \mathbb{R}^m$. Then, the matrix equation $A\vec{x} = \vec{b}$ has precisely the same solution set as the system of linear equations whose augmented matrix is,
\begin{equation*}
    \begin{bmatrix} \vec{a}_1 & \vec{a}_2 & \dots & \vec{a}_n & \vec{b} \end{bmatrix}
\end{equation*}
\end{theorem}

\begin{corollary}
The equation $A\vec{x} = \vec{b}$ has a solution if and only if $\vec{b}$ is a linear combination of the columns of $A$.
\end{corollary}

\begin{theorem}
Let $A$ be an $m \times n$ matrix. Then, the following are equivalent:
\begin{enumerate}[(a)]
    \item The equation $A \vec{x} = \vec{b}$ has a solution for every $\vec{b} \in \mathbb{R}^m$.
    \item Every $\vec{b} \in \mathbb{R}^m$ can be written as a linear combination of the columns of $A$.
    \item The columns of $A$ span $\mathbb{R}^m$.
    \item $A$ has a pivot position in every row.
\end{enumerate}
\end{theorem}

\section*{The Identity Matrix}
\begin{definition}
The $n \times n$ \textbf{identity matrix}, $I_n$, is the matrix with $1$'s along the main diagonal (from top left to bottom right), and $0$'s everywhere else. In other words,
\begin{equation*}
    I_n = \begin{bmatrix} 1 & 0 & \dots & 0 \\ 0 & 1 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 1 \end{bmatrix}
\end{equation*}
\end{definition}

For example, the $2 \times 2$ identity matrix $I_2$, and the $3 \times 3$ identity matrix $I_3$ are given by
\begin{equation*}
    I_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \qquad I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\end{equation*}

The identity matrix has the property that $I_n \vec{x} = \vec{x}$ for all $\vec{x} \in \mathbb{R}^n$.

\section*{Properties of the Matrix-Vector Product}
\begin{theorem}
Let $A$ be a $m \times n$ matrix, $\vec{u}, \vec{v} \in \mathbb{R}^n$ be vectors, $c \in \mathbb{R}$ be a scalar. Then,
\begin{enumerate}[(a)]
    \item $A(\vec{u} + \vec{v}) = A\vec{u} + A\vec{v}$
    \item $A(c\vec{u}) = c(A\vec{u})$.
\end{enumerate}
\end{theorem}

The proof just involves the definition of the matrix-vector product, and the properties of $\mathbb{R}^n$.

\begin{proof}
\textbf{EXERCISE}.
\end{proof}


\end{document}