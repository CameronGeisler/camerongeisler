\documentclass[letterpaper,12pt]{article}
\input{preamble}

\chead{Introduction to Determinants}

\begin{document}

Determinants are an operation on square matrices. The determinant is a number associated with a matrix, which provides information about its invertibility, as well as information about its associated linear transformation.

\section*{Determinants}
One motivation of the determinant is as a criterion for invertibility of a matrix. In the simplest case, for a $1 \times 1$ matrix, say $A = \begin{bmatrix} a \end{bmatrix}$, recall the inverse matrix is $\begin{bmatrix} 1/a \end{bmatrix}$, provided that $a \neq 0$. In this way, we define the determinant of a $1 \times 1$ matrix as the entry itself.

\begin{definition}
For a $1 \times 1$ matrix $A = \begin{bmatrix} a \end{bmatrix}$, the \textbf{determinant} of $A$ is $\det{A} = a$.
\end{definition}

For a $2 \times 2$ matrix, recall that a matrix $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$ is invertible if and only if $ad - bc \neq 0$. This leads to a similarly suitable definition, which is not simple to compute.

\begin{definition}
For a $2 \times 2$ matrix $A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}$, the \textbf{determinant} of $A$, $\det{A}$, is defined by
\begin{equation*}
    \det{A} = ad - bc
\end{equation*}
\begin{itemize}
    \item Determinants can be denoted by the ``function notation" $\det{A}$, or by writing a matrix with vertical bars rather than square brackets, as $\abs{A}$ or $\begin{vmatrix} a & b \\ c & d \end{vmatrix}$.
\end{itemize}
\end{definition}

In other words, it is the ``difference of the products of the diagonals".

\begin{example}
Solve the matrix equation for $x$,
\begin{equation*}
    \begin{vmatrix} -3 & x \\ 7 & 4 \end{vmatrix} = -54
\end{equation*}
Evaluating the determinant on the left hand side,
\begin{align*}
    -3(4) - 7x & = -54 \\
    -12 - 7x & = -54 \\
    -7x & = -42 \\
    x & = 6
\end{align*}
\end{example}

\section*{3rd Order Determinants}
For $3 \times 3$ matrices and larger, evaluating determinants is more complex. Let $A$ be a $3 \times 3$ matrix,
\begin{equation*}
    A = \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \end{bmatrix}
\end{equation*}
Similarly, consider a criterion for invertibility. Recall that a matrix $A$ is invertible if $A$ is row equivalent to $I$. In this way, assume that $A$ is invertible, so that $A$ is row equivalent to $I$. Then, we want to determine what restrictions that places on the entries of $A$. First, assume without loss of generality that $a_{11} \neq 0$ (otherwise, row interchange to get a pivot in the first column). Then, using row reduction, multiply rows 2 and 3 by $a_{11}$, and subtract suitable multiples of row 1, to get
\begin{equation*}
    \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    a_{11} a_{21} & a_{11} a_{22} & a_{11} a_{23} \\
    a_{11} a_{31} & a_{11} a_{32} & a_{11} a_{33} \end{bmatrix} \sim \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    0 & a_{11} a_{22} - a_{12} a_{21} & a_{11} a_{23} - a_{13} a_{21} \\
    0 & a_{11} a_{32} - a_{12} a_{31} & a_{11} a_{33} - a_{13} a_{31} \end{bmatrix}
\end{equation*}
Notice that we normally would have instead done
\begin{align*}
    R_2 -\frac{a_{21}}{a_{11}} R_1 \longrightarrow R_2 \\
    R_3 - \frac{a_{31}}{a_{11}} R_1 \longrightarrow R_3
\end{align*}
which also obtains 0's below the first pivot. However, these operations leads to fractional entries, which just makes the computations more complicated.
\\ \\ Next, if $A$ is invertible, then at least one of the $(2,2)$-entry or the $(3,2)$-entry is non-zero. Similarly, without loss of generality, assume that the $(2,2)$-entry is non-zero (otherwise, use a row interchange). Then, to obtain a 0 in the $(3,2)$-entry, multiply row 3 by $a_{11} a_{22} - a_{12} a_{21}$, and add $-(a_{11} a_{32} - a_{12} a_{31})$ times row 2 to row 3. This results in,
\begin{align*}
    \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    0 & a_{11} a_{22} - a_{12} a_{21} & a_{11} a_{23} - a_{13} a_{21} \\
    0 & (a_{11} a_{32} - a_{12} a_{31})(a_{11} a_{22} - a_{12} a_{21}) & (a_{11} a_{33} - a_{13} a_{31})(a_{11} a_{22} - a_{12} a_{21}) \end{bmatrix}
\end{align*}
and finally,
\begin{equation*}
    \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    0 & a_{11} a_{22} - a_{12} a_{21} & a_{11} a_{23} - a_{13} a_{21} \\
    0 & 0 & (a_{11} a_{33} - a_{13} a_{31})(a_{11} a_{22} - a_{12} a_{21}) - (a_{11} a_{32} - a_{12} a_{31})(a_{11} a_{23} - a_{13} a_{21}) \end{bmatrix}
\end{equation*}
The matrix $A$ is row equivalent to this matrix, so $A$ is row equivalent to $I$ only if the $(3,3)$-entry is non-zero, or
\begin{align*}
    & (a_{11} a_{33} - a_{12} a_{21})(a_{11} a_{22} - a_{12} a_{21}) - (a_{11} a_{22} - a_{12} a_{21})(a_{11} a_{23} - a_{12} a_{21}) \\
    & = a_{11} \brac{a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33} - a_{13} a_{22} a_{31}}
\end{align*}
Since $a_{11} \neq 0$ by assumption, this entry is non-zero if and only if the other factor is non-zero
\begin{equation*}
    a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33} - a_{13} a_{22} a_{31} \neq 0
\end{equation*}
It turns out that the converse is also true, that if this quantity is non-zero, then the matrix $A$ is invertible. For now,

\begin{definition}
Let $A$ be a $3 \times 3$ matrix, The \textbf{determinant} of $A$, $\det{A}$, is defined by,
\begin{equation*}
    \det{A} = a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31} + a_{13} a_{21} a_{32} - a_{11} a_{23} a_{32} - a_{12} a_{21} a_{33} - a_{13} a_{22} a_{31}
\end{equation*}
\end{definition}

This definition is notationally complex and difficult to remember, so there are multiple alternate interpretations which make computations easier.

\section*{Recursive Definition}
Notice that the definition of a $3 \times 3$ matrix can be written in terms of $2 \times 2$ matrices. First, group terms, where each group shares a common factor,
\begin{align*}
     \det{A} & = (a_{11} a_{22} a_{33} - a_{11} a_{23} a_{32}) - (a_{12} a_{21} a_{33} - a_{12} a_{23} a_{31}) + (a_{13} a_{21} a_{32} - a_{13} a_{22} a_{31}) \\
     & = a_{11} (a_{22} a_{33} - a_{23} a_{32}) - a_{12}(a_{21} a_{33} - a_{23} a_{31}) + a_{13} (a_{21} a_{32} - a_{22} a_{31})
\end{align*}
Then, notice that each expression in brackets is the determinant of a particular $2 \times 2$ matrix,
\begin{align*}
    \det{A} & = a_{11} \det{\begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}} - a_{12} \det{\begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix}} + a_{13} \det{\begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix}}
\end{align*}

Notice that these are determinants of particular ``submatrices" of $A$,
% \begin{equation*}
%     \begin{array}{ccc}
%         \det{\begin{bmatrix} a_{22} & a_{23} \\ a_{32} & a_{33} \end{bmatrix}} & \det{\begin{bmatrix} a_{21} & a_{23} \\ a_{31} & a_{33} \end{bmatrix}} & \det{\begin{bmatrix} a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix}} \\[16pt]
%         \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix} & \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix} & \begin{bmatrix} a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23} \\ a_{31} & a_{32} & a_{33} \end{bmatrix}
%     \end{array}
% \end{equation*}

\begin{figure}[H]
    \centering
    \def\svgwidth{0.6\linewidth}
    \input{../images/determinant.pdf_tex}
\end{figure}

Notice that the first submatrix is $A$ with the first row and first column removed. Similarly, the second submatrix is $A$ with the first row and \textit{second} column removed, and the third submatrix is $A$ with the first row and \textit{third} column removed. In general, a \textbf{minor} of a matrix $A$ is a submatrix formed by deleting one row and one column of $A$. The minor of $A$ with row $i$ and column $j$ removed is denoted by $A_{ij}$ (note that this notation is also sometimes used to denote the $(i,j)$-entry of $A$). Then, the 3 submatrices above are $A_{11}, A_{12}$, and $A_{13}$.

\begin{theorem}
\textbf{3x3 determinant equivalent definition}.
\begin{align*}
    \boxed{\det{A} = a_{11} \det{A_{11}} - a_{12} \det{A_{12}} + a_{13} \det{A_{13}}}
\end{align*}
\end{theorem}

This writes the determinant of a $3 \times 3$ matrix in terms of $2 \times 2$ matrices.

\section*{Diagonal Entries Trick for $3 \times 3$ Determinants}
The formula for the determinant of a $3 \times 3$ entries can be remembered using the following trick: first, copy the first two columns to the right of the matrix,
\begin{equation*}
    \begin{bmatrix} a_{11} & a_{12} & a_{13} \\
    a_{21} & a_{22} & a_{23} \\
    a_{31} & a_{32} & a_{33} \end{bmatrix} \begin{matrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{matrix}
\end{equation*}
Then, the determinant of the matrix is the sum of the products on each of the six diagonals,

\begin{figure}[H]
    \centering
    \def\svgwidth{0.4\linewidth}
    \input{../images/determinant-diagonal.pdf_tex}
\end{figure}
using the appropriate sign for each product. Note that this trick does not generalize to larger matrices.


\section*{nth Order Determinants}
The formula for the determinant of an $n$th order matrix can be defined in a similar way.

\begin{definition}
Let $A = \begin{bmatrix} a_{ij} \end{bmatrix}$ be an $n \times n$ matrix. Then, the \textbf{determinant} of $A$ is the sum of $n$ terms of the form $\pm a_{1j} \det{A_{1j}}$, with plus and minus signs alternating, and the entries $a_{11}, a_{12}, \dots, a_{1n}$ are from the first row of $A$,
\begin{align*}
    \boxed{\det{A} = a_{11} \det{A_{11}} - a_{12} \det{A_{12}} + \dots + (-1)^{1+n} a_{1n} \det{A_{1n}}}
\end{align*}
In short,
\begin{equation*}
    \boxed{\det{A} = \sum_{j=1}^n (-1)^{1+j} a_{1j} \det{A_{1j}}}
\end{equation*}
\end{definition}

This formula is sometimes called \textbf{cofactor expansion about the first row}, or \textbf{Laplace expansion}, or \textbf{expansion by minors}.

\begin{definition}
For a matrix $A = \begin{bmatrix} a_{ij} \end{bmatrix}$, the $(i,j)$-\textbf{cofactor} of $A$ is the number $C_{ij}$ defined by,
\begin{equation*}
    C_{ij} = (-1)^{i+j} \det{A_{ij}}
\end{equation*}

Then, the formula for the determinant of an $n \times n$ matrix can be written as
\begin{equation*}
    \det{A} = a_{11} C_{11} + a_{12} C_{12} + \dots + a_{1n} C_{1n}
\end{equation*}
\end{definition}

\section*{Laplace Expansion Along a Row/Column}
In fact, the determinant of an $n \times n$ matrix can be done by doing expansion about \textit{any} row, \textit{or column}, of the matrix.

\begin{theorem}
The determinant of an $n \times n$ matrix $A$ can be computed by cofactor expansion across any row or down any column. The expansion across the $i$th row is given by,
\begin{equation*}
    \det{A} = a_{i1} C_{i1} + a_{i2} C_{i2} + \dots + a_{in} C_{in}
\end{equation*}
The expansion down the $j$th column is given by,
\begin{equation*}
    \det{A} = a_{1j} C_{1j} + a_{2j} C_{2j} + \dots + a_{nj} C_{nj}
\end{equation*}
\end{theorem}

The plus or minus signs in the $(i,j)$-cofactor depends only on the position of $a_{ij}$ in the matrix. The factor $(-1)^{i+j}$ determines a checkerboard pattern,
\begin{equation*}
    \begin{bmatrix} + & - & + & \cdots \\ - & + & - & \\ + & - & + & \\ \vdots & & & \ddots \end{bmatrix}
\end{equation*}

The flexibility of expansion means that to compute a determinant, one can choose the row or column which leads to the most convenient computation. In particular, choosing a row or column with many zero entries results in fewer computations.


\section*{Determinant of Triangular Matrices}

\begin{theorem}
If $A$ is a triangular matrix (either upper triangular or lower triangular), then $\det{A}$ is given by the product of the elements of $A$ on the main diagonal,
\begin{equation*}
    \boxed{\det{A} = a_{11} \cdot \dots \cdot a_{nn}}
\end{equation*}
\end{theorem}

\begin{proof}
See advanced determinants?
\end{proof}

\end{document}