\documentclass[letterpaper,12pt]{article}
\input{preamble}

\chead{Matrix Multiplication}

\begin{document}

Addition and scalar multiplication of matrices is defined in the intuitively natural way, entry-wise. \textit{Matrix multiplication} is not computed entry-wise.

\section*{Matrix Multiplication (Products of Matrices)}
\begin{definition}
Let $A$ be a $m \times n$ matrix, $B$ be a $n \times p$ matrix. Then, the \textbf{product} of $A$ and $B$, $AB$, is the $m \times p$ matrix whose entry in the row $i$ and column $j$ are given by the sum of products of the $i$th row of $A$ and $j$th column of $B$, or
\begin{equation*}
    \boxed{(AB)_{ij} = a_{i1} b_{ij} + a_{i2} b_{2j} + \dots + a_{in} b_{nj} = \sum_{k=1}^n A_{ik} B_{kj}}
\end{equation*}
\end{definition}

In order for the product $AB$ to be defined, the number of columns of $A$ must be equal to the number of rows of $B$. The dimensions ``$(m \times n) \cdot (n \times p) = (m \times p)$" mean that the two ``inner" dimensions must be equal, and the two ``outer" dimensions are the dimensions of the product.

\begin{example}
If $A$ has size $3 \times 1$, $B$ has size $1 \times 4$, then the product $AB$ will have size $3 \times 4$, but the product $BA$ does not exist.
\end{example}

\begin{example}
Determine the product if possible
\begin{equation*}
    \begin{bmatrix} 2 & -2 & 2 \\ 4 & 0 & 2 \end{bmatrix} \begin{bmatrix} -2 \\ 2 \\ 2 \end{bmatrix}
\end{equation*}
The first matrix has dimensions $2 \times 3$, the second has dimensions $3 \times 1$, so the product exists and has dimensions $2 \times 1$.
\begin{align*}
    \begin{bmatrix} 2 & -2 & 2 \\ 4 & 0 & 2 \end{bmatrix} \begin{bmatrix} -2 \\ 2 \\ 2 \end{bmatrix} & = \begin{bmatrix} 2(-2) + (-2)(2) + 2(2) \\ 4(-2) + 0(2) + 2(2) \end{bmatrix} \\
    & = \begin{bmatrix} -4 \\ -4 \end{bmatrix}
\end{align*}
\end{example}

\begin{example}
Determine the product if possible
\begin{equation*}
    \begin{bmatrix} -1 & -3 & -3 \\ 2 & -1 & 5 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ -5 & 0 \\ -4 & 2 \end{bmatrix}
\end{equation*}
The first matrix has dimension $2 \times 3$, the second has dimension $3 \times 2$, so the product exists and has dimensions $2 \times 2$.
\begin{align*}
    \begin{bmatrix} -1 & -3 & -3 \\ 2 & -1 & 5 \end{bmatrix} \begin{bmatrix} 2 & 0 \\ -5 & 0 \\ -4 & 2 \end{bmatrix} & = \begin{bmatrix} -1(2) - 3(-5) - 3(-4) & -1(0) - 3(0) - 3(2) \\ 2(2) - 1(-5) + 5(-4) & 2(0) - 1(0) + 5(2) \end{bmatrix} \\
    & = \begin{bmatrix} 25 & -6 \\ -11 & 10 \end{bmatrix}
\end{align*}
\end{example}

\section*{Matrix Multiplication as Function Composition}
Recall that matrices $A, B$ can be thought of as linear transformations on vectors. In particular, let $A$ be an $m \times n$ matrix, $B$ be an $n \times p$ matrix. Then, first, if $\vec{x} \in \mathbb{R}^p$, then multiplying $\vec{x}$ by $B$ transforms it into the vector $B \vec{x}$ in $\mathbb{R}^n$. Then, further multiplication by $A(B\vec{x})$ is a vector in $\mathbb{R}^m$. In this way, $A(B\vec{x})$ is a composition of two transformations $A$ and $B$. The matrix $AB$ is defined precisely to represent this composition of transformations as a single transformation.
\\ \\ Let the columns of $B$ be $\vec{b}_1, \dots, \vec{b}_p$, $\vec{x}$ have entries $x_1, \dots, x_p$. Then,
\begin{equation*}
    B\vec{x} = x_1 \vec{b}_1 + \dots + x_p \vec{b}_p
\end{equation*}
Then,
\begin{align*}
    A(B\vec{x}) & = A(x_1 \vec{b}_1 + \dots + x_p \vec{b}_p) \\
    & = x_1 (A\vec{b}_1) + \dots + x_p (A\vec{b}_p) && \text{by linearity of the matrix-vector product}
\end{align*}
In other words, the vector $A(B\vec{x})$ is the linear combination of $A\vec{b}_1, \dots, A\vec{b}_p$ with the entries of $\vec{x}$ as weights. In matrix notation,
\begin{align*}
    A(B\vec{x}) & = \begin{bmatrix} A \vec{b}_1 & \cdots & A \vec{b}_p \end{bmatrix} \vec{x}
\end{align*}
Thus, the matrix $\begin{bmatrix} A \vec{b}_1 & \cdots & A \vec{b}_p \end{bmatrix}$ is precisely the matrix which transforms $\vec{x}$ into $A(B\vec{x})$. Then, the $(i,j)$-entry of this matrix is the $i$th entry of $A\vec{b}_j$, which is precisely the sum of products of the corresponding entries from row $i$ of $A$ and the vector $\vec{b}_j$, which is $(AB)_{ij}$. In summary,

\begin{theorem}
Let $A$ be an $m \times n$ matrix, $B$ be an $n \times p$ matrix with columns $\vec{b}_1, \dots, \vec{b}_p$. Then, their product $AB$ is the $m \times p$ matrix with columns $A\vec{b}_1, \dots, A\vec{b}_p$, or
\begin{equation*}
    \boxed{AB = A \begin{bmatrix} \vec{b}_1 & \cdots & \vec{b}_p \end{bmatrix} = \begin{bmatrix} A \vec{b}_1 & \cdots & A \vec{b}_p \end{bmatrix}}
\end{equation*}
\end{theorem}

Recall that linear transformations can always be represented by a standard matrix. Then, a direct consequence is that compositions of linear transformations are linear transformations.

\section*{Properties of Matrix Multiplication}
Matrix multiplication shares many properties of real number multiplication, assuming the matrices have the appropriate dimensions.

\begin{theorem}
Let $A$ be an $m \times n$ matrix, $B$ and $C$ be $n \times p$ matrices, $D$ and $E$ be $q \times m$ matrices, $a \in \mathbb{R}$. Then,
\begin{enumerate}[(a)]
    \item \textbf{Multiplication distributes over addition}.
    \begin{equation*}
        A(B + C) = AB + AC \qquad \text{and} \qquad (D + E)A = DA + EA
    \end{equation*}
    \item \textbf{Associative with scalar multiplication}.
    \begin{equation*}
        a(AB) = (aA)B = A(aB)
    \end{equation*}
    \item \textbf{Associative with matrix multiplication}.
    \begin{equation*}
        A(BC) = (AB)C
    \end{equation*}
\end{enumerate}
\end{theorem}

These properties intuitively means that parentheses surrounding matrix expressions generally work the same as for real numbers.

\begin{proof}
\begin{align*}
    (A(B + C))_{ij} & = \sum_{k=1}^n A_{ik}(B + C)_{kj} && \text{definition of matrix multiplication} \\
    & = \sum_{k=1}^n A_{ik}(B_{kj} + C_{kj}) && \text{definition of matrix addition} \\
    & = \sum_{k=1}^n (A_{ik} B_{kj} + A_{ik} C_{kj}) \\
    & = \sum_{k=1}^n A_{ik}B_{kj} + \sum_{k=1}^n A_{ik}C_{kj} \\
    & = (AB)_{ij} + (AC)_{ij} && \text{definition of matrix multiplication} \\
    & = (AB + AC)_{ij} && \text{definition of matrix addition}
\end{align*}
\end{proof}

\section*{Matrix Multiplication is not Commutative}
Matrix multiplication, unlike multiplication of real numbers, is not commutative, in that in general, $AB \neq BA$.
\\ \\ In fact, for a $m \times n$ matrix $A$ and $n \times p$ matrix $B$, while $AB$ is defined like above, $BA$ is not defined, unless $m = p$. If $m = p$, then while $AB$ has dimensions $m \times m$ like above, $BA$ will have dimensions $n \times n$, and matrices with different dimension cannot be equal, so $AB \neq BA$. The exception is if $m = n$, i.e. both matrices are square. Even in this special case, matrix multiplication is still not commutative in general. If $AB = BA$ for particular matrices $A, B$, then we say that $A$ and $B$ \textbf{commute}.
\\ \\ Intuitively, representing matrices as transformations, matrix multiplication is not commutative because function composition is not commutative.
\\ \\ More concretely, $AB$ is a linear combination of the columns of $B$, whereas $BA$ is a linear combination of the columns of $A$, and there is no reason to expect these to be equal.
\\ \\ Then, in the product $AB$, we sometimes say $A$ is \textbf{right-multiplied} by $B$, or $B$ is \textbf{left-multiplied} by $A$.

\section*{Identity Matrix}
Recall that for real numbers, multiplying a number by 1 results in the number being unchanged. In other words, 1 is the \textit{multiplicative identity} for multiplication of real numbers, $a \cdot 1 = a$ for all $a \in \mathbb{R}$. Similarly, we can define a matrix which, when multiplied by a matrix, leaves it unchanged, called an \textit{identity matrix}.

\begin{definition}
The $n \times n$ \textbf{identity matrix}, $I_n$, is the matrix with $1$'s along the main diagonal (from top left to bottom right), and $0$'s everywhere else. In other words,
\begin{equation*}
    I_n = \begin{bmatrix} 1 & 0 & \dots & 0 \\ 0 & 1 & \dots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \dots & 1 \end{bmatrix}
\end{equation*}
\end{definition}

For example, the $2 \times 2$ identity matrix $I_2$, and the $3 \times 3$ identity matrix $I_3$ are given by
\begin{equation*}
    I_2 = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix} \qquad I_3 = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}
\end{equation*}

The definition of an identity matrix can be simplified by defining a symbol called the Kronecker delta. 

\begin{definition}
The \textbf{Kronecker delta}, $\delta_{ij}$, is defined to be
\begin{align*}
    \delta_{ij} = \begin{cases} 1 & \text{if $i = j$} \\ 0 & \text{if $i \neq j$} \end{cases}
\end{align*}
\end{definition}

For example, $\delta_{12} = 0$, and $\delta_{55} = 1$. Then, the $(i,j)$-th entry of the identity matrix is given by
\begin{equation*}
    (I_n)_{ij} = \delta_{ij}
\end{equation*}

\section*{Product of Matrix and Identity Matrix}
The identity matrix is the multiplicative identity for matrices, in that the product of any matrix and the appropriate identity matrix is simply the matrix itself. This is analogous to the number 1 in arithmetic.

\begin{theorem}
Let $A$ be an $m \times n$ matrix. Then,
\begin{equation*}
    I_m A = A = AI_n
\end{equation*}
\end{theorem}

\begin{proof}
\begin{align*}
    (I_m A)_{ij} & = \sum_{k=1}^n (I_m)_{ik} A_{kj} && \text{definition of matrix multiplication} \\
    & = \sum_{k=1}^n \delta_{ik} A_{kj} && \text{definition of identity matrix} \\
    & = \delta_{ii} A_{ij} \\
    & = A_{ij}
\end{align*}
\begin{align*}
    (A I_n)_{ij} & = \sum_{k=1}^n A_{ik} (I_n)_{kj} && \text{definition of matrix multiplication} \\
    & = \sum_{k=1}^n A_{ik} \delta_{kj} && \text{definition of identity matrix} \\
    & = A_{ij} \delta_{jj} \\
    & = A_{ij}
\end{align*}
\end{proof}

\section*{More Properties}
The \textit{cancellation laws} do not hold for matrix multiplication, in that if $AB = AC$, then it is not true in general that $B = C$.
\\ \\ Also, if a product of matrices is equal to the zero matrix, i.e. $AB = 0$, then it is not true that either $A = 0$ or $B = 0$.

\section*{Powers of a Matrix}
\begin{definition}
Let $A$ be an $n \times n$ matrix, $k \in \mathbb{N}$. Then, $A^k$ denotes the product of $k$ copies of $A$,
\begin{equation*}
    A^k = \underbrace{A \cdots A}_{\text{$k$ times}}
\end{equation*}
For $k = 0$, $A^0$ is defined as the identity matrix.
\end{definition}

Then, the basic power laws apply,

\begin{theorem}
Let $A$ be an $n \times n$ matrix, $k,j \in \mathbb{N}$.
\begin{enumerate}[(a)]
    \item $A^k \cdot A^j = A^{k+j}$.
    \item $(A^k)^j = A^{kj}$.
\end{enumerate}
\end{theorem}

These just follow directly from the definition.

\begin{proof}
First, $A^k$ is the product of $k$ copies of $A$, and $A^j$ is the product of $j$ copies of $A$, so $A^k \cdot A^j$ is the product of $k + j$ copies of $A$, or $A^{k+j}$. Also, $(A^k)^j$ is the product of $j$ copies of $A^k$, which is the product of $kj$ copies of $A$, or $A^{kj}$.
\end{proof}














\section*{Misc}
\begin{theorem}
Let $V$ be an $n$-dimensional vector space, $B \subseteq V$ be an ordered basis of $V$. Then,
\begin{equation*}
    [\operatorname{Id}_{V}]_{B} = I_n
\end{equation*}
\end{theorem}

\end{document}